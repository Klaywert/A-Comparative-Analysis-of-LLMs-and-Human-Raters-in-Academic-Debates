{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3524db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando a Análise Final e Completa ---\n",
      "\n",
      "[ETAPA 1/6] Lendo e processando os arquivos JSON dos LLMs...\n",
      ">>> Leitura de 12259 registros brutos concluída.\n",
      "\n",
      "[ETAPA 2/6] Carregando arquivos de Ground Truth...\n",
      ">>> Dados de Ground Truth carregados com sucesso.\n",
      "\n",
      "[ETAPA 3/6] Preparando e padronizando rankings...\n",
      ">>> Rankings dos LLMs e Ground Truth preparados e padronizados.\n",
      "\n",
      "[ETAPA 4/6] Calculando as métricas finais...\n",
      "\n",
      "[ETAPA 5/6] Tabela de Resultados Finais:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>winners_accuracy</th>\n",
       "      <th>debaters_accuracy</th>\n",
       "      <th>mrr</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>kendall_tau</th>\n",
       "      <th>spearman_rho</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comparison</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">vs_Judges_HardVote</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.925408</td>\n",
       "      <td>0.592781</td>\n",
       "      <td>0.674839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>0.953680</td>\n",
       "      <td>0.630921</td>\n",
       "      <td>0.680794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.689583</td>\n",
       "      <td>0.908007</td>\n",
       "      <td>0.336921</td>\n",
       "      <td>0.365731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.951635</td>\n",
       "      <td>0.618296</td>\n",
       "      <td>0.715615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.929081</td>\n",
       "      <td>0.500940</td>\n",
       "      <td>0.570866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.931477</td>\n",
       "      <td>0.576630</td>\n",
       "      <td>0.635609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.919716</td>\n",
       "      <td>0.520181</td>\n",
       "      <td>0.598676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.710714</td>\n",
       "      <td>0.781333</td>\n",
       "      <td>0.476835</td>\n",
       "      <td>0.528962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.870884</td>\n",
       "      <td>0.186197</td>\n",
       "      <td>0.200352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.752186</td>\n",
       "      <td>0.505294</td>\n",
       "      <td>0.608390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.857898</td>\n",
       "      <td>0.688715</td>\n",
       "      <td>0.764547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.917397</td>\n",
       "      <td>0.434516</td>\n",
       "      <td>0.488749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">vs_Judges_SoftVote</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.904995</td>\n",
       "      <td>0.490388</td>\n",
       "      <td>0.615543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.776042</td>\n",
       "      <td>0.932367</td>\n",
       "      <td>0.522632</td>\n",
       "      <td>0.616557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.910756</td>\n",
       "      <td>0.462120</td>\n",
       "      <td>0.517909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.927046</td>\n",
       "      <td>0.523721</td>\n",
       "      <td>0.653043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.692708</td>\n",
       "      <td>0.907318</td>\n",
       "      <td>0.415388</td>\n",
       "      <td>0.528043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.713542</td>\n",
       "      <td>0.915760</td>\n",
       "      <td>0.540388</td>\n",
       "      <td>0.615543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.903890</td>\n",
       "      <td>0.466854</td>\n",
       "      <td>0.570331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>0.698810</td>\n",
       "      <td>0.768232</td>\n",
       "      <td>0.484549</td>\n",
       "      <td>0.537134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.226182</td>\n",
       "      <td>0.240007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.751438</td>\n",
       "      <td>0.618455</td>\n",
       "      <td>0.712836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.847582</td>\n",
       "      <td>0.651969</td>\n",
       "      <td>0.749912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.899806</td>\n",
       "      <td>0.460805</td>\n",
       "      <td>0.518862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">vs_SelfAssessment</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.959993</td>\n",
       "      <td>0.677253</td>\n",
       "      <td>0.740074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.968887</td>\n",
       "      <td>0.666712</td>\n",
       "      <td>0.716817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.943413</td>\n",
       "      <td>0.443403</td>\n",
       "      <td>0.488809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.981128</td>\n",
       "      <td>0.668246</td>\n",
       "      <td>0.733459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.946991</td>\n",
       "      <td>0.541759</td>\n",
       "      <td>0.624963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.978526</td>\n",
       "      <td>0.574968</td>\n",
       "      <td>0.651661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.947174</td>\n",
       "      <td>0.668639</td>\n",
       "      <td>0.743943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.743056</td>\n",
       "      <td>0.781139</td>\n",
       "      <td>0.472643</td>\n",
       "      <td>0.550546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.920469</td>\n",
       "      <td>0.450270</td>\n",
       "      <td>0.486735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.762991</td>\n",
       "      <td>0.708428</td>\n",
       "      <td>0.782424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.856090</td>\n",
       "      <td>0.686348</td>\n",
       "      <td>0.752682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.970814</td>\n",
       "      <td>0.606391</td>\n",
       "      <td>0.682121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  winners_accuracy  debaters_accuracy  \\\n",
       "comparison         prompt model                                         \n",
       "vs_Judges_HardVote 1      claude          0.555556           0.411765   \n",
       "                          gemini          0.705882           0.455882   \n",
       "                          gpt4o           0.444444           0.382353   \n",
       "                   2      claude          0.588235           0.411765   \n",
       "                          gemini          0.529412           0.397059   \n",
       "                          gpt4o           0.529412           0.411765   \n",
       "                   3      claude          0.500000           0.323529   \n",
       "                          gemini          0.444444           0.355932   \n",
       "                          gpt4o           0.333333           0.279412   \n",
       "                   4      claude          0.444444           0.375000   \n",
       "                          gemini          0.588235           0.375000   \n",
       "                          gpt4o           0.555556           0.455882   \n",
       "vs_Judges_SoftVote 1      claude          0.470588           0.397059   \n",
       "                          gemini          0.625000           0.455882   \n",
       "                          gpt4o           0.529412           0.426471   \n",
       "                   2      claude          0.500000           0.426471   \n",
       "                          gemini          0.500000           0.411765   \n",
       "                          gpt4o           0.500000           0.485294   \n",
       "                   3      claude          0.529412           0.352941   \n",
       "                          gemini          0.470588           0.406780   \n",
       "                          gpt4o           0.391304           0.264706   \n",
       "                   4      claude          0.470588           0.392857   \n",
       "                          gemini          0.562500           0.453125   \n",
       "                          gpt4o           0.529412           0.470588   \n",
       "vs_SelfAssessment  1      claude          0.722222           0.433333   \n",
       "                          gemini          0.647059           0.466667   \n",
       "                          gpt4o           0.500000           0.333333   \n",
       "                   2      claude          0.764706           0.433333   \n",
       "                          gemini          0.529412           0.300000   \n",
       "                          gpt4o           0.705882           0.366667   \n",
       "                   3      claude          0.600000           0.416667   \n",
       "                          gemini          0.388889           0.333333   \n",
       "                          gpt4o           0.461538           0.483333   \n",
       "                   4      claude          0.529412           0.375000   \n",
       "                          gemini          0.529412           0.339286   \n",
       "                          gpt4o           0.631579           0.500000   \n",
       "\n",
       "                                       mrr      ndcg  kendall_tau  \\\n",
       "comparison         prompt model                                     \n",
       "vs_Judges_HardVote 1      claude  0.791667  0.925408     0.592781   \n",
       "                          gemini  0.848958  0.953680     0.630921   \n",
       "                          gpt4o   0.689583  0.908007     0.336921   \n",
       "                   2      claude  0.802083  0.951635     0.618296   \n",
       "                          gemini  0.734375  0.929081     0.500940   \n",
       "                          gpt4o   0.734375  0.931477     0.576630   \n",
       "                   3      claude  0.750000  0.919716     0.520181   \n",
       "                          gemini  0.710714  0.781333     0.476835   \n",
       "                          gpt4o   0.729167  0.870884     0.186197   \n",
       "                   4      claude  0.794872  0.752186     0.505294   \n",
       "                          gemini  0.811111  0.857898     0.688715   \n",
       "                          gpt4o   0.765625  0.917397     0.434516   \n",
       "vs_Judges_SoftVote 1      claude  0.718750  0.904995     0.490388   \n",
       "                          gemini  0.776042  0.932367     0.522632   \n",
       "                          gpt4o   0.731250  0.910756     0.462120   \n",
       "                   2      claude  0.729167  0.927046     0.523721   \n",
       "                          gemini  0.692708  0.907318     0.415388   \n",
       "                          gpt4o   0.713542  0.915760     0.540388   \n",
       "                   3      claude  0.739583  0.903890     0.466854   \n",
       "                          gemini  0.698810  0.768232     0.484549   \n",
       "                          gpt4o   0.760417  0.850610     0.226182   \n",
       "                   4      claude  0.794872  0.751438     0.618455   \n",
       "                          gemini  0.772222  0.847582     0.651969   \n",
       "                          gpt4o   0.744792  0.899806     0.460805   \n",
       "vs_SelfAssessment  1      claude  0.964286  0.959993     0.677253   \n",
       "                          gemini  0.892857  0.968887     0.666712   \n",
       "                          gpt4o   0.821429  0.943413     0.443403   \n",
       "                   2      claude  0.964286  0.981128     0.668246   \n",
       "                          gemini  0.797619  0.946991     0.541759   \n",
       "                          gpt4o   0.928571  0.978526     0.574968   \n",
       "                   3      claude  0.880952  0.947174     0.668639   \n",
       "                          gemini  0.743056  0.781139     0.472643   \n",
       "                          gpt4o   0.880952  0.920469     0.450270   \n",
       "                   4      claude  0.909091  0.762991     0.708428   \n",
       "                          gemini  0.833333  0.856090     0.686348   \n",
       "                          gpt4o   0.928571  0.970814     0.606391   \n",
       "\n",
       "                                  spearman_rho  \n",
       "comparison         prompt model                 \n",
       "vs_Judges_HardVote 1      claude      0.674839  \n",
       "                          gemini      0.680794  \n",
       "                          gpt4o       0.365731  \n",
       "                   2      claude      0.715615  \n",
       "                          gemini      0.570866  \n",
       "                          gpt4o       0.635609  \n",
       "                   3      claude      0.598676  \n",
       "                          gemini      0.528962  \n",
       "                          gpt4o       0.200352  \n",
       "                   4      claude      0.608390  \n",
       "                          gemini      0.764547  \n",
       "                          gpt4o       0.488749  \n",
       "vs_Judges_SoftVote 1      claude      0.615543  \n",
       "                          gemini      0.616557  \n",
       "                          gpt4o       0.517909  \n",
       "                   2      claude      0.653043  \n",
       "                          gemini      0.528043  \n",
       "                          gpt4o       0.615543  \n",
       "                   3      claude      0.570331  \n",
       "                          gemini      0.537134  \n",
       "                          gpt4o       0.240007  \n",
       "                   4      claude      0.712836  \n",
       "                          gemini      0.749912  \n",
       "                          gpt4o       0.518862  \n",
       "vs_SelfAssessment  1      claude      0.740074  \n",
       "                          gemini      0.716817  \n",
       "                          gpt4o       0.488809  \n",
       "                   2      claude      0.733459  \n",
       "                          gemini      0.624963  \n",
       "                          gpt4o       0.651661  \n",
       "                   3      claude      0.743943  \n",
       "                          gemini      0.550546  \n",
       "                          gpt4o       0.486735  \n",
       "                   4      claude      0.782424  \n",
       "                          gemini      0.752682  \n",
       "                          gpt4o       0.682121  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ETAPA 6/6] Salvando tabela de resultados...\n",
      "\n",
      "Tabela de resultados finais salva em 'final_summary_results_full.csv'\n",
      "\n",
      "--- Análise concluída! ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"--- Iniciando a Análise Final e Completa ---\")\n",
    "\n",
    "# --- 1. CONFIGURAÇÃO ---\n",
    "OUTPUTS_DIR = 'outputs'\n",
    "SELF_ASSESSMENT_PATH = 'gt_self_assessment_ranking.csv'\n",
    "JUDGES_HARD_VOTE_PATH = 'gt_judges_hard_vote_ranking.csv'\n",
    "JUDGES_SOFT_VOTE_PATH = 'gt_judges_soft_vote_ranking.csv'\n",
    "\n",
    "# --- 2. CARREGAMENTO E PROCESSAMENTO DOS DADOS BRUTOS DOS LLMs ---\n",
    "print(\"\\n[ETAPA 1/6] Lendo e processando os arquivos JSON dos LLMs...\")\n",
    "all_runs_data = []\n",
    "try:\n",
    "    for root, dirs, files in os.walk(OUTPUTS_DIR):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    parts = file_path.split(os.sep)\n",
    "                    prompt_num = int(parts[1].split('_')[1])\n",
    "                    debate_num = int(parts[2].split('_')[1])\n",
    "                    model_name = filename.split('_')[0]\n",
    "                    run_num = int(filename.split('_')[2].split('.')[0])\n",
    "                except (ValueError, IndexError): continue\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                if \"error\" in data: continue\n",
    "\n",
    "                for debater in data.get('debaters', []):\n",
    "                    debater_name = debater.get('name')\n",
    "                    performance_analysis = debater.get('performance', {}).get('performance_analysis', debater.get('performance_evaluation', ''))\n",
    "                    \n",
    "                    if prompt_num == 1:\n",
    "                        score = debater.get('overall_score')\n",
    "                        if score is not None:\n",
    "                            all_runs_data.append({'prompt': 1, 'debate': debate_num, 'model': model_name, 'run': run_num, 'debater': debater_name, 'criterion': 'overall_score', 'score': score, 'analysis_text': performance_analysis})\n",
    "                    elif prompt_num == 3:\n",
    "                        positive_events = debater.get('positive_events', {})\n",
    "                        negative_events = debater.get('negative_events', {})\n",
    "                        score = sum(len(v) for v in positive_events.values()) - sum(len(v) for v in negative_events.values())\n",
    "                        all_runs_data.append({'prompt': 3, 'debate': debate_num, 'model': model_name, 'run': run_num, 'debater': debater_name, 'criterion': 'total_event_score', 'score': score, 'analysis_text': performance_analysis})\n",
    "                    else: # Prompts 2 e 4\n",
    "                        scores_data = debater.get('scores', {})\n",
    "                        if not scores_data: scores_data = {k: v.get('score') for k, v in debater.get('evaluation_aspects', {}).items()}\n",
    "                        for criterion, score in scores_data.items():\n",
    "                            all_runs_data.append({'prompt': prompt_num, 'debate': debate_num, 'model': model_name, 'run': run_num, 'debater': debater_name, 'criterion': criterion, 'score': score, 'analysis_text': performance_analysis})\n",
    "    \n",
    "    raw_df = pd.DataFrame(all_runs_data)\n",
    "    print(f\">>> Leitura de {len(raw_df)} registros brutos concluída.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO na leitura dos JSONs: {e}\")\n",
    "\n",
    "# --- 3. CARREGAMENTO DOS DADOS DE GROUND TRUTH ---\n",
    "print(\"\\n[ETAPA 2/6] Carregando arquivos de Ground Truth...\")\n",
    "try:\n",
    "    self_assessment_ranking_df = pd.read_csv(SELF_ASSESSMENT_PATH)\n",
    "    judges_hard_vote_ranking_df = pd.read_csv(JUDGES_HARD_VOTE_PATH)\n",
    "    judges_soft_vote_ranking_df = pd.read_csv(JUDGES_SOFT_VOTE_PATH)\n",
    "    print(\">>> Dados de Ground Truth carregados com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao carregar arquivos de Ground Truth: {e}\")\n",
    "\n",
    "# --- 4. PREPARAÇÃO E PADRONIZAÇÃO FINAL DOS RANKINGS ---\n",
    "print(\"\\n[ETAPA 3/6] Preparando e padronizando rankings...\")\n",
    "try:\n",
    "    def standardize_debater_name(df, col_name='debater'):\n",
    "        if col_name in df.columns:\n",
    "            df[col_name] = df[col_name].astype(str).str.upper().str.replace(' ', '_').str.replace('DEBATER_', 'DEBATER_')\n",
    "        return df\n",
    "\n",
    "    raw_df = standardize_debater_name(raw_df)\n",
    "    self_assessment_ranking_df = standardize_debater_name(self_assessment_ranking_df)\n",
    "    judges_hard_vote_ranking_df = standardize_debater_name(judges_hard_vote_ranking_df)\n",
    "    judges_soft_vote_ranking_df = standardize_debater_name(judges_soft_vote_ranking_df)\n",
    "\n",
    "    agg_df = raw_df.groupby(['prompt', 'debate', 'model', 'debater', 'criterion'])['score'].mean().reset_index()\n",
    "    total_scores = agg_df.groupby(['prompt', 'debate', 'model', 'debater'])['score'].sum().reset_index()\n",
    "    total_scores['rank'] = total_scores.groupby(['prompt', 'debate', 'model'])['score'].rank(method='dense', ascending=False).astype(int)\n",
    "    final_llm_rankings = total_scores.sort_values(by=['prompt', 'debate', 'model', 'rank'])\n",
    "    print(\">>> Rankings dos LLMs e Ground Truth preparados e padronizados.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro na preparação dos rankings: {e}\")\n",
    "\n",
    "# --- 5. FUNÇÕES DE MÉTRICA ---\n",
    "def calculate_winners_accuracy(predictions, ground_truth):\n",
    "    pred_winners = predictions[predictions['rank'] == 1]\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(pred_winners, gt_winners[['debate', 'gt_debater']], on='debate')\n",
    "    correct = (merged['debater'] == merged['gt_debater'])\n",
    "    return correct.groupby([merged['prompt'], merged['model']]).mean().rename('winners_accuracy')\n",
    "\n",
    "def calculate_debaters_accuracy(predictions, ground_truth):\n",
    "    merged = pd.merge(predictions, ground_truth, on=['debate', 'debater'], suffixes=('_pred', '_gt'))\n",
    "    merged['is_correct'] = (merged['rank_pred'] == merged['rank_gt'])\n",
    "    return merged.groupby(['prompt', 'model'])['is_correct'].mean().rename('debaters_accuracy')\n",
    "\n",
    "def calculate_mrr(predictions, ground_truth):\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(predictions, gt_winners[['debate', 'gt_debater']], on='debate', how='left')\n",
    "    correct_predictions = merged[merged['debater'] == merged['gt_debater']]\n",
    "    first_correct_rank = correct_predictions.groupby(['prompt', 'debate', 'model'])['rank'].min()\n",
    "    return (1 / first_correct_rank).groupby(['prompt', 'model']).mean().rename('mrr')\n",
    "\n",
    "def calculate_ndcg(predictions, ground_truth):\n",
    "    ground_truth['relevance'] = 1 / ground_truth['rank']\n",
    "    merged = pd.merge(predictions, ground_truth[['debate', 'debater', 'relevance']], on=['debate', 'debater'], how='left').fillna(0)\n",
    "    results = {}\n",
    "    for name, group in merged.groupby(['prompt', 'model', 'debate']):\n",
    "        prompt, model, debate = name\n",
    "        if len(group) < 2: continue\n",
    "        true_relevance = np.asarray([group.sort_values(by='relevance', ascending=False)['relevance'].values])\n",
    "        predicted_relevance = np.asarray([group.sort_values(by='rank')['relevance'].values])\n",
    "        ndcg = ndcg_score(true_relevance, predicted_relevance)\n",
    "        if (prompt, model) not in results: results[(prompt, model)] = []\n",
    "        results[(prompt, model)].append(ndcg)\n",
    "    final_ndcg = {k: np.mean(v) for k, v in results.items()}\n",
    "    return pd.Series(final_ndcg, name='ndcg').rename_axis(['prompt', 'model'])\n",
    "\n",
    "def calculate_rank_correlation(predictions, ground_truth, method='kendall'):\n",
    "    merged = pd.merge(predictions, ground_truth, on=['debate', 'debater'], suffixes=('_pred', '_gt'))\n",
    "    correlations = []\n",
    "    for name, group in merged.groupby(['prompt', 'model', 'debate']):\n",
    "        if len(group['rank_pred']) == len(group['rank_gt']) and len(group) > 1:\n",
    "            if method == 'kendall': corr, _ = kendalltau(group['rank_pred'], group['rank_gt'])\n",
    "            elif method == 'spearman': corr, _ = spearmanr(group['rank_pred'], group['rank_gt'])\n",
    "            else: corr = np.nan\n",
    "            correlations.append({'prompt': name[0], 'model': name[1], 'correlation': corr})\n",
    "    corr_df = pd.DataFrame(correlations)\n",
    "    return corr_df.groupby(['prompt', 'model'])['correlation'].mean()\n",
    "\n",
    "\n",
    "# --- 6. EXECUÇÃO FINAL E EXIBIÇÃO ---\n",
    "print(\"\\n[ETAPA 4/6] Calculando as métricas finais...\")\n",
    "ground_truths = {\n",
    "    \"vs_SelfAssessment\": self_assessment_ranking_df,\n",
    "    \"vs_Judges_HardVote\": judges_hard_vote_ranking_df,\n",
    "    \"vs_Judges_SoftVote\": judges_soft_vote_ranking_df\n",
    "}\n",
    "final_results_list = []\n",
    "\n",
    "for gt_name, gt_df in ground_truths.items():\n",
    "    common_debates = gt_df['debate'].unique()\n",
    "    predictions_filtered = final_llm_rankings[final_llm_rankings['debate'].isin(common_debates)]\n",
    "    \n",
    "    acc = calculate_winners_accuracy(predictions_filtered, gt_df)\n",
    "    debaters_acc = calculate_debaters_accuracy(predictions_filtered, gt_df)\n",
    "    mrr = calculate_mrr(predictions_filtered, gt_df)\n",
    "    ndcg = calculate_ndcg(predictions_filtered, gt_df)\n",
    "    kendall = calculate_rank_correlation(predictions_filtered, gt_df, method='kendall').rename('kendall_tau')\n",
    "    spearman = calculate_rank_correlation(predictions_filtered, gt_df, method='spearman').rename('spearman_rho')\n",
    "    \n",
    "    result_df = pd.concat([acc, debaters_acc, mrr, ndcg, kendall, spearman], axis=1)\n",
    "    result_df['comparison'] = gt_name\n",
    "    final_results_list.append(result_df)\n",
    "\n",
    "final_summary_df = pd.concat(final_results_list).reset_index()\n",
    "final_summary_df = final_summary_df.set_index(['comparison', 'prompt', 'model']).sort_index()\n",
    "\n",
    "print(\"\\n[ETAPA 5/6] Tabela de Resultados Finais:\")\n",
    "display(final_summary_df)\n",
    "\n",
    "print(\"\\n[ETAPA 6/6] Salvando tabela de resultados...\")\n",
    "final_summary_df.to_csv('final_summary_results_full.csv')\n",
    "print(\"\\nTabela de resultados finais salva em 'final_summary_results_full.csv'\")\n",
    "print(\"\\n--- Análise concluída! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11b778a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Comparação: Autoavaliação vs. Juízes (com todas as métricas) ---\n",
      ">>> Arquivos de Ground Truth carregados com sucesso.\n",
      "\n",
      "[ETAPA 2/2] Calculando as métricas de comparação...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Klaywert\\AppData\\Local\\Temp\\ipykernel_668\\1204565364.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ground_truth['relevance'] = 1 / ground_truth['rank']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTADOS FINAIS: AUTOAVALIAÇÃO vs. JUÍZES (COM TODAS AS MÉTRICAS) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Klaywert\\AppData\\Local\\Temp\\ipykernel_668\\1204565364.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ground_truth['relevance'] = 1 / ground_truth['rank']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winners_accuracy</th>\n",
       "      <th>debaters_accuracy</th>\n",
       "      <th>mrr</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>kendall_tau</th>\n",
       "      <th>spearman_rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vs_Judges_HardVote</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.954860</td>\n",
       "      <td>0.636508</td>\n",
       "      <td>0.693943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vs_Judges_SoftVote</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.939015</td>\n",
       "      <td>0.582101</td>\n",
       "      <td>0.659188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    winners_accuracy  debaters_accuracy       mrr      ndcg  \\\n",
       "vs_Judges_HardVote          0.555556           0.550000  0.845238  0.954860   \n",
       "vs_Judges_SoftVote          0.529412           0.316667  0.809524  0.939015   \n",
       "\n",
       "                    kendall_tau  spearman_rho  \n",
       "vs_Judges_HardVote     0.636508      0.693943  \n",
       "vs_Judges_SoftVote     0.582101      0.659188  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import kendalltau, spearmanr # <-- Adicionado import\n",
    "\n",
    "print(\"--- Iniciando Comparação: Autoavaliação vs. Juízes (com todas as métricas) ---\")\n",
    "\n",
    "# --- 1. CARREGAMENTO DOS DADOS DE GROUND TRUTH ---\n",
    "try:\n",
    "    self_assessment_df = pd.read_csv('gt_self_assessment_ranking.csv')\n",
    "    judges_hard_vote_df = pd.read_csv('gt_judges_hard_vote_ranking.csv')\n",
    "    judges_soft_vote_df = pd.read_csv('gt_judges_soft_vote_ranking.csv')\n",
    "    print(\">>> Arquivos de Ground Truth carregados com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Não foi possível carregar os arquivos .csv. Verifique se eles existem. Erro: {e}\")\n",
    "\n",
    "# --- 2. FUNÇÕES PARA CÁLCULO DAS MÉTRICAS ---\n",
    "\n",
    "def calculate_winners_accuracy(predictions, ground_truth):\n",
    "    pred_winners = predictions[predictions['rank'] == 1]\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(pred_winners, gt_winners[['debate', 'gt_debater']], on='debate')\n",
    "    correct = (merged['debater'] == merged['gt_debater'])\n",
    "    return correct.mean()\n",
    "\n",
    "def calculate_debaters_accuracy(predictions, ground_truth):\n",
    "    merged = pd.merge(predictions, ground_truth, on=['debate', 'debater'], suffixes=('_pred', '_gt'))\n",
    "    merged['is_correct'] = (merged['rank_pred'] == merged['rank_gt'])\n",
    "    return merged['is_correct'].mean()\n",
    "\n",
    "def calculate_mrr(predictions, ground_truth):\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(predictions, gt_winners[['debate', 'gt_debater']], on='debate')\n",
    "    correct_predictions = merged[merged['debater'] == merged['gt_debater']]\n",
    "    first_correct_rank = correct_predictions.groupby('debate')['rank'].min()\n",
    "    mrr = (1 / first_correct_rank).mean()\n",
    "    return mrr\n",
    "\n",
    "def calculate_ndcg(predictions, ground_truth):\n",
    "    ground_truth['relevance'] = 1 / ground_truth['rank']\n",
    "    merged = pd.merge(predictions, ground_truth[['debate', 'debater', 'relevance']], on=['debate', 'debater'], how='left').fillna(0)\n",
    "    ndcg_scores = []\n",
    "    for debate_id, group in merged.groupby('debate'):\n",
    "        if len(group) < 2: continue\n",
    "        true_relevance = np.asarray([group.sort_values(by='relevance', ascending=False)['relevance'].values])\n",
    "        predicted_relevance = np.asarray([group.sort_values(by='rank')['relevance'].values])\n",
    "        ndcg_scores.append(ndcg_score(true_relevance, predicted_relevance))\n",
    "    return np.mean(ndcg_scores) if ndcg_scores else np.nan\n",
    "\n",
    "# --- NOVA FUNÇÃO ADICIONADA AQUI ---\n",
    "def calculate_rank_correlation(predictions, ground_truth, method='kendall'):\n",
    "    merged = pd.merge(predictions, ground_truth, on=['debate', 'debater'], suffixes=('_pred', '_gt'))\n",
    "    correlations = []\n",
    "    for debate_id, group in merged.groupby('debate'):\n",
    "        if len(group) > 1: # Precisa de mais de 1 item para calcular correlação\n",
    "            if method == 'kendall':\n",
    "                corr, _ = kendalltau(group['rank_pred'], group['rank_gt'])\n",
    "            elif method == 'spearman':\n",
    "                corr, _ = spearmanr(group['rank_pred'], group['rank_gt'])\n",
    "            else:\n",
    "                corr = np.nan\n",
    "            correlations.append(corr)\n",
    "    return np.mean(correlations) if correlations else np.nan\n",
    "\n",
    "\n",
    "# --- 3. EXECUÇÃO DAS COMPARAÇÕES ---\n",
    "print(\"\\n[ETAPA 2/2] Calculando as métricas de comparação...\")\n",
    "\n",
    "results = {}\n",
    "common_debates = self_assessment_df['debate'].unique()\n",
    "judges_hard_filtered = judges_hard_vote_df[judges_hard_vote_df['debate'].isin(common_debates)]\n",
    "judges_soft_filtered = judges_soft_vote_df[judges_soft_vote_df['debate'].isin(common_debates)]\n",
    "\n",
    "# Adicionando Debaters Accuracy e as novas métricas de correlação\n",
    "results['vs_Judges_HardVote'] = {\n",
    "    'winners_accuracy': calculate_winners_accuracy(self_assessment_df, judges_hard_filtered),\n",
    "    'debaters_accuracy': calculate_debaters_accuracy(self_assessment_df, judges_hard_filtered),\n",
    "    'mrr': calculate_mrr(self_assessment_df, judges_hard_filtered),\n",
    "    'ndcg': calculate_ndcg(self_assessment_df, judges_hard_filtered),\n",
    "    'kendall_tau': calculate_rank_correlation(self_assessment_df, judges_hard_filtered, method='kendall'),\n",
    "    'spearman_rho': calculate_rank_correlation(self_assessment_df, judges_hard_filtered, method='spearman')\n",
    "}\n",
    "\n",
    "results['vs_Judges_SoftVote'] = {\n",
    "    'winners_accuracy': calculate_winners_accuracy(self_assessment_df, judges_soft_filtered),\n",
    "    'debaters_accuracy': calculate_debaters_accuracy(self_assessment_df, judges_soft_filtered),\n",
    "    'mrr': calculate_mrr(self_assessment_df, judges_soft_filtered),\n",
    "    'ndcg': calculate_ndcg(self_assessment_df, judges_soft_filtered),\n",
    "    'kendall_tau': calculate_rank_correlation(self_assessment_df, judges_soft_filtered, method='kendall'),\n",
    "    'spearman_rho': calculate_rank_correlation(self_assessment_df, judges_soft_filtered, method='spearman')\n",
    "}\n",
    "\n",
    "# --- 4. EXIBIÇÃO DOS RESULTADOS ---\n",
    "summary_df = pd.DataFrame(results).T\n",
    "print(\"\\n--- RESULTADOS FINAIS: AUTOAVALIAÇÃO vs. JUÍZES (COM TODAS AS MÉTRICAS) ---\")\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0a6054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando a Análise Final e Cálculo de Métricas ---\n",
      "\n",
      "[ETAPA 1/4] Carregando todos os dataframes...\n",
      ">>> Dados carregados com sucesso.\n",
      "\n",
      "[ETAPA 2/4] Preparando e padronizando os rankings...\n",
      ">>> Rankings dos LLMs e Ground Truth preparados e padronizados.\n",
      "\n",
      "[ETAPA 3/4] Calculando as métricas finais...\n",
      "\n",
      "[ETAPA 4/4] Tabela de Resultados Finais:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>winners_accuracy</th>\n",
       "      <th>mrr</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comparison</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">vs_Judges_HardVote</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.982091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.925408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>0.953680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.689583</td>\n",
       "      <td>0.908007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.951635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.929081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.931477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.752186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.857898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.917397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">vs_Judges_SoftVote</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.904995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.776042</td>\n",
       "      <td>0.932367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.910756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.927046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.692708</td>\n",
       "      <td>0.907318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.713542</td>\n",
       "      <td>0.915760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.751438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.847582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.899806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">vs_SelfAssessment</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.942622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.945544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.953326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.932175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.964037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.817708</td>\n",
       "      <td>0.944069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.958577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.773167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.855767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.951829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  winners_accuracy       mrr      ndcg\n",
       "comparison         prompt model                                       \n",
       "vs_Judges_HardVote 0.0    0                    NaN       NaN  0.982091\n",
       "                   1.0    claude          0.555556  0.791667  0.925408\n",
       "                          gemini          0.705882  0.848958  0.953680\n",
       "                          gpt4o           0.444444  0.689583  0.908007\n",
       "                   2.0    claude          0.588235  0.802083  0.951635\n",
       "                          gemini          0.529412  0.734375  0.929081\n",
       "                          gpt4o           0.529412  0.734375  0.931477\n",
       "                   4.0    claude          0.444444  0.794872  0.752186\n",
       "                          gemini          0.588235  0.811111  0.857898\n",
       "                          gpt4o           0.555556  0.765625  0.917397\n",
       "vs_Judges_SoftVote 0.0    0                    NaN       NaN  1.000000\n",
       "                   1.0    claude          0.470588  0.718750  0.904995\n",
       "                          gemini          0.625000  0.776042  0.932367\n",
       "                          gpt4o           0.529412  0.731250  0.910756\n",
       "                   2.0    claude          0.500000  0.729167  0.927046\n",
       "                          gemini          0.500000  0.692708  0.907318\n",
       "                          gpt4o           0.500000  0.713542  0.915760\n",
       "                   4.0    claude          0.470588  0.794872  0.751438\n",
       "                          gemini          0.562500  0.772222  0.847582\n",
       "                          gpt4o           0.529412  0.744792  0.899806\n",
       "vs_SelfAssessment  0.0    0                    NaN       NaN  0.942622\n",
       "                   1.0    claude          0.583333  0.921875  0.945544\n",
       "                          gemini          0.521739  0.859375  0.953326\n",
       "                          gpt4o           0.375000  0.770833  0.932175\n",
       "                   2.0    claude          0.608696  0.921875  0.964037\n",
       "                          gemini          0.478261  0.817708  0.944069\n",
       "                          gpt4o           0.565217  0.890625  0.958577\n",
       "                   4.0    claude          0.400000  0.865385  0.773167\n",
       "                          gemini          0.391304  0.777778  0.855767\n",
       "                          gpt4o           0.520000  0.890625  0.951829"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabela de resultados finais salva em 'final_summary_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "print(\"--- Iniciando a Análise Final e Cálculo de Métricas ---\")\n",
    "\n",
    "# --- 1. CARREGAMENTO DE TODOS OS DADOS PREPARADOS ---\n",
    "print(\"\\n[ETAPA 1/4] Carregando todos os dataframes...\")\n",
    "try:\n",
    "    llm_scores_df = pd.read_csv('aggregated_results.csv')\n",
    "    llm_winners_p1_df = pd.read_csv('aggregated_results_prompt_1_winners.csv')\n",
    "    self_assessment_ranking_df = pd.read_csv('gt_self_assessment_ranking.csv')\n",
    "    judges_hard_vote_ranking_df = pd.read_csv('gt_judges_hard_vote_ranking.csv')\n",
    "    judges_soft_vote_ranking_df = pd.read_csv('gt_judges_soft_vote_ranking.csv')\n",
    "    print(\">>> Dados carregados com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Arquivo não encontrado. Verifique se todos os arquivos .csv estão na pasta. Detalhe: {e}\")\n",
    "\n",
    "# --- 2. PREPARAÇÃO E PADRONIZAÇÃO DOS RANKINGS ---\n",
    "print(\"\\n[ETAPA 2/4] Preparando e padronizando os rankings...\")\n",
    "try:\n",
    "    # Padroniza nomes dos debatedores em todos os DataFrames para 'DEBATER_X'\n",
    "    def standardize_debater_name(df, col_name='debater'):\n",
    "        if col_name in df.columns:\n",
    "            df[col_name] = df[col_name].astype(str).str.upper().str.replace(' ', '_')\n",
    "        return df\n",
    "\n",
    "    llm_scores_df = standardize_debater_name(llm_scores_df)\n",
    "    for col in llm_winners_p1_df.columns:\n",
    "        if col.lower() != 'debate':\n",
    "            llm_winners_p1_df[col] = standardize_debater_name(llm_winners_p1_df, col)\n",
    "            \n",
    "    self_assessment_ranking_df = standardize_debater_name(self_assessment_ranking_df)\n",
    "    judges_hard_vote_ranking_df = standardize_debater_name(judges_hard_vote_ranking_df)\n",
    "    judges_soft_vote_ranking_df = standardize_debater_name(judges_soft_vote_ranking_df)\n",
    "\n",
    "    # Prepara rankings dos LLMs para prompts 2, 3 e 4\n",
    "    llm_total_scores = llm_scores_df.groupby(['prompt', 'debate', 'model', 'debater'])['score'].sum().reset_index()\n",
    "    llm_total_scores['rank'] = llm_total_scores.groupby(['prompt', 'debate', 'model'])['score'].rank(method='dense', ascending=False).astype(int)\n",
    "    llm_ranking_p234 = llm_total_scores.sort_values(by=['prompt', 'debate', 'model', 'rank'])\n",
    "\n",
    "    # Prepara rankings dos LLMs para prompt 1\n",
    "    llm_winners_p1_long = llm_winners_p1_df.melt(id_vars='debate', var_name='model', value_name='debater')\n",
    "    llm_winners_p1_long['prompt'] = 1\n",
    "    llm_winners_p1_long = standardize_debater_name(llm_winners_p1_long)\n",
    "\n",
    "    all_debaters = judges_soft_vote_ranking_df[['debate', 'debater']].drop_duplicates()\n",
    "    llm_winners_p1_ranked = pd.merge(all_debaters, llm_winners_p1_long, on=['debate', 'debater'], how='left')\n",
    "    llm_winners_p1_ranked['rank'] = np.where(llm_winners_p1_ranked['model'].notna(), 1, 2)\n",
    "    \n",
    "    final_llm_rankings = pd.concat([llm_winners_p1_ranked, llm_ranking_p234])\n",
    "    print(\">>> Rankings dos LLMs e Ground Truth preparados e padronizados.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro na preparação dos rankings: {e}\")\n",
    "\n",
    "# --- 3. FUNÇÕES PARA CÁLCULO DAS MÉTRICAS ---\n",
    "def calculate_winners_accuracy(predictions, ground_truth):\n",
    "    pred_winners = predictions[predictions['rank'] == 1]\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(pred_winners, gt_winners[['debate', 'gt_debater']], on='debate')\n",
    "    correct = (merged['debater'] == merged['gt_debater'])\n",
    "    accuracy = correct.groupby([merged['prompt'], merged['model']]).mean().rename('winners_accuracy')\n",
    "    return accuracy\n",
    "\n",
    "def calculate_mrr(predictions, ground_truth):\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(predictions, gt_winners[['debate', 'gt_debater']], on='debate', how='left')\n",
    "    correct_predictions = merged[merged['debater'] == merged['gt_debater']]\n",
    "    first_correct_rank = correct_predictions.groupby(['prompt', 'debate', 'model'])['rank'].min()\n",
    "    mrr = (1 / first_correct_rank).groupby(['prompt', 'model']).mean().rename('mrr')\n",
    "    return mrr\n",
    "\n",
    "def calculate_ndcg(predictions, ground_truth):\n",
    "    ground_truth['relevance'] = 1 / ground_truth['rank']\n",
    "    merged = pd.merge(predictions, ground_truth[['debate', 'debater', 'relevance']], on=['debate', 'debater'], how='left').fillna(0)\n",
    "    \n",
    "    results = {}\n",
    "    for name, group in merged.groupby(['prompt', 'model', 'debate']):\n",
    "        prompt, model, debate = name\n",
    "        if len(group) < 2: continue\n",
    "        \n",
    "        true_relevance = np.asarray([group.sort_values(by='relevance', ascending=False)['relevance'].values])\n",
    "        predicted_relevance = np.asarray([group.sort_values(by='rank')['relevance'].values])\n",
    "        \n",
    "        ndcg = ndcg_score(true_relevance, predicted_relevance)\n",
    "        if (prompt, model) not in results: results[(prompt, model)] = []\n",
    "        results[(prompt, model)].append(ndcg)\n",
    "        \n",
    "    final_ndcg = {k: np.mean(v) for k, v in results.items()}\n",
    "    return pd.Series(final_ndcg, name='ndcg').rename_axis(['prompt', 'model'])\n",
    "\n",
    "# --- 4. EXECUÇÃO DAS ANÁLISES ---\n",
    "print(\"\\n[ETAPA 3/4] Calculando as métricas finais...\")\n",
    "\n",
    "ground_truths = {\n",
    "    \"vs_SelfAssessment\": self_assessment_ranking_df,\n",
    "    \"vs_Judges_HardVote\": judges_hard_vote_ranking_df,\n",
    "    \"vs_Judges_SoftVote\": judges_soft_vote_ranking_df\n",
    "}\n",
    "final_results_list = []\n",
    "\n",
    "for gt_name, gt_df in ground_truths.items():\n",
    "    common_debates = gt_df['debate'].unique()\n",
    "    predictions_filtered = final_llm_rankings[final_llm_rankings['debate'].isin(common_debates)]\n",
    "        \n",
    "    acc = calculate_winners_accuracy(predictions_filtered, gt_df)\n",
    "    mrr = calculate_mrr(predictions_filtered, gt_df)\n",
    "    ndcg = calculate_ndcg(predictions_filtered, gt_df)\n",
    "    \n",
    "    result_df = pd.concat([acc, mrr, ndcg], axis=1)\n",
    "    result_df['comparison'] = gt_name\n",
    "    final_results_list.append(result_df)\n",
    "\n",
    "final_summary_df = pd.concat(final_results_list).reset_index()\n",
    "final_summary_df = final_summary_df.set_index(['comparison', 'prompt', 'model']).sort_index()\n",
    "\n",
    "print(\"\\n[ETAPA 4/4] Tabela de Resultados Finais:\")\n",
    "display(final_summary_df)\n",
    "\n",
    "final_summary_df.to_csv('final_summary_results.csv')\n",
    "print(\"\\nTabela de resultados finais salva em 'final_summary_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5c1747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando a Análise Final e Cálculo de Métricas (v2 - Completa) ---\n",
      "\n",
      "[ETAPA 1/5] Carregando todos os dataframes...\n",
      ">>> Dados carregados com sucesso.\n",
      "\n",
      "[ETAPA 2/5] Preparando e padronizando os rankings...\n",
      ">>> Rankings dos LLMs preparados.\n",
      "\n",
      "[ETAPA 3/4] Calculando as métricas finais...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Klaywert\\AppData\\Local\\Temp\\ipykernel_24976\\3109728343.py:51: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  llm_winners_p1_ranked['model'] = llm_winners_p1_ranked.groupby(['debate'])['model'].ffill().bfill()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ETAPA 4/4] Tabela de Resultados Finais:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>winners_accuracy</th>\n",
       "      <th>debaters_accuracy</th>\n",
       "      <th>mrr</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comparison</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">vs_Judges_HardVote</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.982091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.925408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>0.953680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.689583</td>\n",
       "      <td>0.908007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.951635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.929081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.931477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.752186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.857898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.917397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">vs_Judges_SoftVote</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.904995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.776042</td>\n",
       "      <td>0.932367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.910756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.927046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.692708</td>\n",
       "      <td>0.907318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.713542</td>\n",
       "      <td>0.915760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.751438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.847582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.899806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">vs_SelfAssessment</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.942622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.945544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.521739</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.953326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.932175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.964037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.478261</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.817708</td>\n",
       "      <td>0.944069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.958577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.773167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.391304</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.855767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.520000</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.951829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  winners_accuracy  debaters_accuracy  \\\n",
       "comparison         prompt model                                         \n",
       "vs_Judges_HardVote 0.0    0                    NaN                NaN   \n",
       "                   1.0    claude          0.555556                5.6   \n",
       "                          gemini          0.705882                6.2   \n",
       "                          gpt4o           0.444444                5.2   \n",
       "                   2.0    claude          0.588235                5.6   \n",
       "                          gemini          0.529412                5.4   \n",
       "                          gpt4o           0.529412                5.6   \n",
       "                   4.0    claude          0.444444                4.2   \n",
       "                          gemini          0.588235                4.8   \n",
       "                          gpt4o           0.555556                6.2   \n",
       "vs_Judges_SoftVote 0.0    0                    NaN                NaN   \n",
       "                   1.0    claude          0.470588                5.4   \n",
       "                          gemini          0.625000                6.2   \n",
       "                          gpt4o           0.529412                5.8   \n",
       "                   2.0    claude          0.500000                5.8   \n",
       "                          gemini          0.500000                5.6   \n",
       "                          gpt4o           0.500000                6.6   \n",
       "                   4.0    claude          0.470588                4.4   \n",
       "                          gemini          0.562500                5.8   \n",
       "                          gpt4o           0.529412                6.4   \n",
       "vs_SelfAssessment  0.0    0                    NaN                NaN   \n",
       "                   1.0    claude          0.583333                5.4   \n",
       "                          gemini          0.521739                5.8   \n",
       "                          gpt4o           0.375000                4.2   \n",
       "                   2.0    claude          0.608696                5.4   \n",
       "                          gemini          0.478261                4.6   \n",
       "                          gpt4o           0.565217                4.8   \n",
       "                   4.0    claude          0.400000                3.6   \n",
       "                          gemini          0.391304                4.0   \n",
       "                          gpt4o           0.520000                6.4   \n",
       "\n",
       "                                       mrr      ndcg  \n",
       "comparison         prompt model                       \n",
       "vs_Judges_HardVote 0.0    0            NaN  0.982091  \n",
       "                   1.0    claude  0.791667  0.925408  \n",
       "                          gemini  0.848958  0.953680  \n",
       "                          gpt4o   0.689583  0.908007  \n",
       "                   2.0    claude  0.802083  0.951635  \n",
       "                          gemini  0.734375  0.929081  \n",
       "                          gpt4o   0.734375  0.931477  \n",
       "                   4.0    claude  0.794872  0.752186  \n",
       "                          gemini  0.811111  0.857898  \n",
       "                          gpt4o   0.765625  0.917397  \n",
       "vs_Judges_SoftVote 0.0    0            NaN  1.000000  \n",
       "                   1.0    claude  0.718750  0.904995  \n",
       "                          gemini  0.776042  0.932367  \n",
       "                          gpt4o   0.731250  0.910756  \n",
       "                   2.0    claude  0.729167  0.927046  \n",
       "                          gemini  0.692708  0.907318  \n",
       "                          gpt4o   0.713542  0.915760  \n",
       "                   4.0    claude  0.794872  0.751438  \n",
       "                          gemini  0.772222  0.847582  \n",
       "                          gpt4o   0.744792  0.899806  \n",
       "vs_SelfAssessment  0.0    0            NaN  0.942622  \n",
       "                   1.0    claude  0.921875  0.945544  \n",
       "                          gemini  0.859375  0.953326  \n",
       "                          gpt4o   0.770833  0.932175  \n",
       "                   2.0    claude  0.921875  0.964037  \n",
       "                          gemini  0.817708  0.944069  \n",
       "                          gpt4o   0.890625  0.958577  \n",
       "                   4.0    claude  0.865385  0.773167  \n",
       "                          gemini  0.777778  0.855767  \n",
       "                          gpt4o   0.890625  0.951829  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabela de resultados finais salva em 'final_summary_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "print(\"--- Iniciando a Análise Final e Cálculo de Métricas (v2 - Completa) ---\")\n",
    "\n",
    "# --- 1. CARREGAMENTO DE TODOS OS DADOS PREPARADOS ---\n",
    "print(\"\\n[ETAPA 1/5] Carregando todos os dataframes...\")\n",
    "try:\n",
    "    llm_scores_df = pd.read_csv('aggregated_results.csv')\n",
    "    llm_winners_p1_df = pd.read_csv('aggregated_results_prompt_1_winners.csv')\n",
    "    self_assessment_ranking_df = pd.read_csv('gt_self_assessment_ranking.csv')\n",
    "    judges_hard_vote_ranking_df = pd.read_csv('gt_judges_hard_vote_ranking.csv')\n",
    "    judges_soft_vote_ranking_df = pd.read_csv('gt_judges_soft_vote_ranking.csv')\n",
    "    print(\">>> Dados carregados com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Arquivo não encontrado. Verifique se todos os arquivos .csv estão na pasta. Detalhe: {e}\")\n",
    "\n",
    "# --- 2. PREPARAÇÃO E PADRONIZAÇÃO DOS RANKINGS ---\n",
    "print(\"\\n[ETAPA 2/5] Preparando e padronizando os rankings...\")\n",
    "try:\n",
    "    def standardize_debater_name(df, col_name='debater'):\n",
    "        if col_name in df.columns:\n",
    "            df[col_name] = df[col_name].astype(str).str.upper().str.replace(' ', '_').str.replace('_', '_', regex=False)\n",
    "        return df\n",
    "\n",
    "    llm_scores_df = standardize_debater_name(llm_scores_df)\n",
    "    # Padroniza nomes de debatedores no llm_winners_p1_df\n",
    "    id_vars = ['debate']\n",
    "    value_vars = [col for col in llm_winners_p1_df.columns if col != 'debate']\n",
    "    llm_winners_p1_df_long = llm_winners_p1_df.melt(id_vars=id_vars, value_vars=value_vars, var_name='model', value_name='debater')\n",
    "    llm_winners_p1_df_long = standardize_debater_name(llm_winners_p1_df_long)\n",
    "    llm_winners_p1_df = llm_winners_p1_df_long.pivot(index='debate', columns='model', values='debater').reset_index()\n",
    "\n",
    "\n",
    "    self_assessment_ranking_df = standardize_debater_name(self_assessment_ranking_df)\n",
    "    judges_hard_vote_ranking_df = standardize_debater_name(judges_hard_vote_ranking_df)\n",
    "    judges_soft_vote_ranking_df = standardize_debater_name(judges_soft_vote_ranking_df)\n",
    "\n",
    "    # Prepara rankings dos LLMs para prompts 2, 3 e 4\n",
    "    llm_total_scores = llm_scores_df.groupby(['prompt', 'debate', 'model', 'debater'])['score'].sum().reset_index()\n",
    "    llm_total_scores['rank'] = llm_total_scores.groupby(['prompt', 'debate', 'model'])['score'].rank(method='dense', ascending=False).astype(int)\n",
    "    llm_ranking_p234 = llm_total_scores.sort_values(by=['prompt', 'debate', 'model', 'rank'])\n",
    "\n",
    "    # Prepara rankings dos LLMs para prompt 1\n",
    "    llm_winners_p1_long = llm_winners_p1_df.melt(id_vars='debate', var_name='model', value_name='debater')\n",
    "    llm_winners_p1_long['prompt'] = 1\n",
    "\n",
    "    all_debaters = judges_soft_vote_ranking_df[['debate', 'debater']].drop_duplicates()\n",
    "    llm_winners_p1_ranked = pd.merge(all_debaters, llm_winners_p1_long, on=['debate', 'debater'], how='left')\n",
    "    llm_winners_p1_ranked['model'] = llm_winners_p1_ranked.groupby(['debate'])['model'].ffill().bfill()\n",
    "    llm_winners_p1_ranked['rank'] = np.where(llm_winners_p1_ranked['debater'].isin(llm_winners_p1_long['debater']), 1, 2)\n",
    "\n",
    "    final_llm_rankings = pd.concat([\n",
    "        llm_winners_p1_ranked[['prompt', 'debate', 'model', 'debater', 'rank']], \n",
    "        llm_ranking_p234[['prompt', 'debate', 'model', 'debater', 'rank']]\n",
    "    ])\n",
    "    print(\">>> Rankings dos LLMs preparados.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro na preparação dos rankings dos LLMs: {e}\")\n",
    "\n",
    "# --- 3. FUNÇÕES PARA CÁLCULO DAS MÉTRICAS ---\n",
    "\n",
    "def calculate_winners_accuracy(predictions, ground_truth):\n",
    "    pred_winners = predictions[predictions['rank'] == 1]\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(pred_winners, gt_winners[['debate', 'gt_debater']], on='debate')\n",
    "    correct = (merged['debater'] == merged['gt_debater'])\n",
    "    accuracy = correct.groupby([merged['prompt'], merged['model']]).mean().rename('winners_accuracy')\n",
    "    return accuracy\n",
    "    \n",
    "# --- NOVA FUNÇÃO ADICIONADA AQUI ---\n",
    "def calculate_debaters_accuracy(predictions, ground_truth):\n",
    "    # Junta as predições e o gabarito\n",
    "    merged = pd.merge(predictions, ground_truth, on=['debate', 'debater'], suffixes=('_pred', '_gt'))\n",
    "    \n",
    "    # Conta onde o rank previsto é igual ao rank real\n",
    "    correct_ranks = (merged['rank_pred'] == merged['rank_gt'])\n",
    "    \n",
    "    # A acurácia é a proporção de acertos em relação ao total de debatedores no gabarito\n",
    "    accuracy_by_group = correct_ranks.groupby([merged['prompt'], merged['model']]).sum()\n",
    "    total_debaters_by_group = merged.groupby(['prompt', 'model'])['debater'].nunique()\n",
    "    \n",
    "    # Para calcular a acurácia geral, precisamos do total de comparações possíveis\n",
    "    # Total de debatedores no ground truth\n",
    "    total_debaters_gt = len(ground_truth)\n",
    "    # Número de prompts e modelos\n",
    "    num_prompts = predictions['prompt'].nunique()\n",
    "    num_models = predictions['model'].nunique()\n",
    "    \n",
    "    # Acurácia total\n",
    "    total_correct = correct_ranks.sum()\n",
    "    # Acurácia por grupo\n",
    "    accuracy = accuracy_by_group / total_debaters_by_group\n",
    "\n",
    "    return accuracy.rename('debaters_accuracy')\n",
    "\n",
    "\n",
    "def calculate_mrr(predictions, ground_truth):\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(predictions, gt_winners[['debate', 'gt_debater']], on='debate', how='left')\n",
    "    correct_predictions = merged[merged['debater'] == merged['gt_debater']]\n",
    "    first_correct_rank = correct_predictions.groupby(['prompt', 'debate', 'model'])['rank'].min()\n",
    "    mrr = (1 / first_correct_rank).groupby(['prompt', 'model']).mean().rename('mrr')\n",
    "    return mrr\n",
    "\n",
    "def calculate_ndcg(predictions, ground_truth):\n",
    "    ground_truth['relevance'] = 1 / ground_truth['rank']\n",
    "    merged = pd.merge(predictions, ground_truth[['debate', 'debater', 'relevance']], on=['debate', 'debater'], how='left').fillna(0)\n",
    "    \n",
    "    results = {}\n",
    "    for name, group in merged.groupby(['prompt', 'model', 'debate']):\n",
    "        prompt, model, debate = name\n",
    "        if len(group) < 2: continue\n",
    "        \n",
    "        true_relevance = np.asarray([group.sort_values(by='relevance', ascending=False)['relevance'].values])\n",
    "        predicted_relevance = np.asarray([group.sort_values(by='rank')['relevance'].values])\n",
    "        \n",
    "        ndcg = ndcg_score(true_relevance, predicted_relevance)\n",
    "        if (prompt, model) not in results: results[(prompt, model)] = []\n",
    "        results[(prompt, model)].append(ndcg)\n",
    "        \n",
    "    final_ndcg = {k: np.mean(v) for k, v in results.items()}\n",
    "    return pd.Series(final_ndcg, name='ndcg').rename_axis(['prompt', 'model'])\n",
    "\n",
    "# --- 4. EXECUÇÃO DAS ANÁLISES ---\n",
    "print(\"\\n[ETAPA 3/4] Calculando as métricas finais...\")\n",
    "\n",
    "ground_truths = {\n",
    "    \"vs_SelfAssessment\": self_assessment_ranking_df,\n",
    "    \"vs_Judges_HardVote\": judges_hard_vote_ranking_df,\n",
    "    \"vs_Judges_SoftVote\": judges_soft_vote_ranking_df\n",
    "}\n",
    "final_results_list = []\n",
    "\n",
    "for gt_name, gt_df in ground_truths.items():\n",
    "    common_debates = gt_df['debate'].unique()\n",
    "    predictions_filtered = final_llm_rankings[final_llm_rankings['debate'].isin(common_debates)]\n",
    "        \n",
    "    acc = calculate_winners_accuracy(predictions_filtered, gt_df)\n",
    "    # --- CHAMADA DA NOVA FUNÇÃO ---\n",
    "    debaters_acc = calculate_debaters_accuracy(predictions_filtered, gt_df)\n",
    "    mrr = calculate_mrr(predictions_filtered, gt_df)\n",
    "    ndcg = calculate_ndcg(predictions_filtered, gt_df)\n",
    "    \n",
    "    # --- ADIÇÃO DA NOVA MÉTRICA AO RESULTADO ---\n",
    "    result_df = pd.concat([acc, debaters_acc, mrr, ndcg], axis=1)\n",
    "    result_df['comparison'] = gt_name\n",
    "    final_results_list.append(result_df)\n",
    "\n",
    "final_summary_df = pd.concat(final_results_list).reset_index()\n",
    "final_summary_df = final_summary_df.set_index(['comparison', 'prompt', 'model']).sort_index()\n",
    "\n",
    "print(\"\\n[ETAPA 4/4] Tabela de Resultados Finais:\")\n",
    "display(final_summary_df)\n",
    "\n",
    "final_summary_df.to_csv('final_summary_results.csv')\n",
    "print(\"\\nTabela de resultados finais salva em 'final_summary_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f991d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando a Análise Final e Cálculo de Métricas (v3 - Debaters Accuracy Corrigido) ---\n",
      "\n",
      "[ETAPA 1/5] Carregando todos os dataframes...\n",
      ">>> Dados carregados com sucesso.\n",
      "\n",
      "[ETAPA 2/5] Preparando e padronizando os rankings...\n",
      ">>> Rankings dos LLMs preparados.\n",
      "\n",
      "[ETAPA 3/4] Calculando as métricas finais...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Klaywert\\AppData\\Local\\Temp\\ipykernel_24976\\3153826701.py:48: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  llm_winners_p1_ranked['model'] = llm_winners_p1_ranked.groupby(['debate'])['model'].ffill().bfill()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ETAPA 4/4] Tabela de Resultados Finais:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>winners_accuracy</th>\n",
       "      <th>debaters_accuracy</th>\n",
       "      <th>mrr</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comparison</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">vs_Judges_HardVote</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.982091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.925408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>0.953680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.689583</td>\n",
       "      <td>0.908007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.951635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.929081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.931477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.752186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.857898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.917397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">vs_Judges_SoftVote</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.904995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.776042</td>\n",
       "      <td>0.932367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.910756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.927046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.692708</td>\n",
       "      <td>0.907318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.713542</td>\n",
       "      <td>0.915760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.751438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.847582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.899806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">vs_SelfAssessment</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.942622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.945544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.953326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.932175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.964037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.338235</td>\n",
       "      <td>0.817708</td>\n",
       "      <td>0.944069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.958577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.773167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.855767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.951829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  winners_accuracy  debaters_accuracy  \\\n",
       "comparison         prompt model                                         \n",
       "vs_Judges_HardVote 0.0    0                    NaN                NaN   \n",
       "                   1.0    claude          0.555556           0.411765   \n",
       "                          gemini          0.705882           0.455882   \n",
       "                          gpt4o           0.444444           0.382353   \n",
       "                   2.0    claude          0.588235           0.411765   \n",
       "                          gemini          0.529412           0.397059   \n",
       "                          gpt4o           0.529412           0.411765   \n",
       "                   4.0    claude          0.444444           0.375000   \n",
       "                          gemini          0.588235           0.375000   \n",
       "                          gpt4o           0.555556           0.455882   \n",
       "vs_Judges_SoftVote 0.0    0                    NaN                NaN   \n",
       "                   1.0    claude          0.470588           0.397059   \n",
       "                          gemini          0.625000           0.455882   \n",
       "                          gpt4o           0.529412           0.426471   \n",
       "                   2.0    claude          0.500000           0.426471   \n",
       "                          gemini          0.500000           0.411765   \n",
       "                          gpt4o           0.500000           0.485294   \n",
       "                   4.0    claude          0.470588           0.392857   \n",
       "                          gemini          0.562500           0.453125   \n",
       "                          gpt4o           0.529412           0.470588   \n",
       "vs_SelfAssessment  0.0    0                    NaN                NaN   \n",
       "                   1.0    claude          0.583333           0.397059   \n",
       "                          gemini          0.521739           0.426471   \n",
       "                          gpt4o           0.375000           0.308824   \n",
       "                   2.0    claude          0.608696           0.397059   \n",
       "                          gemini          0.478261           0.338235   \n",
       "                          gpt4o           0.565217           0.352941   \n",
       "                   4.0    claude          0.400000           0.321429   \n",
       "                          gemini          0.391304           0.312500   \n",
       "                          gpt4o           0.520000           0.470588   \n",
       "\n",
       "                                       mrr      ndcg  \n",
       "comparison         prompt model                       \n",
       "vs_Judges_HardVote 0.0    0            NaN  0.982091  \n",
       "                   1.0    claude  0.791667  0.925408  \n",
       "                          gemini  0.848958  0.953680  \n",
       "                          gpt4o   0.689583  0.908007  \n",
       "                   2.0    claude  0.802083  0.951635  \n",
       "                          gemini  0.734375  0.929081  \n",
       "                          gpt4o   0.734375  0.931477  \n",
       "                   4.0    claude  0.794872  0.752186  \n",
       "                          gemini  0.811111  0.857898  \n",
       "                          gpt4o   0.765625  0.917397  \n",
       "vs_Judges_SoftVote 0.0    0            NaN  1.000000  \n",
       "                   1.0    claude  0.718750  0.904995  \n",
       "                          gemini  0.776042  0.932367  \n",
       "                          gpt4o   0.731250  0.910756  \n",
       "                   2.0    claude  0.729167  0.927046  \n",
       "                          gemini  0.692708  0.907318  \n",
       "                          gpt4o   0.713542  0.915760  \n",
       "                   4.0    claude  0.794872  0.751438  \n",
       "                          gemini  0.772222  0.847582  \n",
       "                          gpt4o   0.744792  0.899806  \n",
       "vs_SelfAssessment  0.0    0            NaN  0.942622  \n",
       "                   1.0    claude  0.921875  0.945544  \n",
       "                          gemini  0.859375  0.953326  \n",
       "                          gpt4o   0.770833  0.932175  \n",
       "                   2.0    claude  0.921875  0.964037  \n",
       "                          gemini  0.817708  0.944069  \n",
       "                          gpt4o   0.890625  0.958577  \n",
       "                   4.0    claude  0.865385  0.773167  \n",
       "                          gemini  0.777778  0.855767  \n",
       "                          gpt4o   0.890625  0.951829  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabela de resultados finais salva em 'final_summary_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "print(\"--- Iniciando a Análise Final e Cálculo de Métricas (v3 - Debaters Accuracy Corrigido) ---\")\n",
    "\n",
    "# --- 1. CARREGAMENTO DE TODOS OS DADOS PREPARADOS ---\n",
    "print(\"\\n[ETAPA 1/5] Carregando todos os dataframes...\")\n",
    "try:\n",
    "    llm_scores_df = pd.read_csv('aggregated_results.csv')\n",
    "    llm_winners_p1_df = pd.read_csv('aggregated_results_prompt_1_winners.csv')\n",
    "    self_assessment_ranking_df = pd.read_csv('gt_self_assessment_ranking.csv')\n",
    "    judges_hard_vote_ranking_df = pd.read_csv('gt_judges_hard_vote_ranking.csv')\n",
    "    judges_soft_vote_ranking_df = pd.read_csv('gt_judges_soft_vote_ranking.csv')\n",
    "    print(\">>> Dados carregados com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Arquivo não encontrado. Verifique se todos os arquivos .csv estão na pasta. Detalhe: {e}\")\n",
    "\n",
    "# --- 2. PREPARAÇÃO E PADRONIZAÇÃO DOS RANKINGS ---\n",
    "print(\"\\n[ETAPA 2/5] Preparando e padronizando os rankings...\")\n",
    "try:\n",
    "    def standardize_debater_name(df, col_name='debater'):\n",
    "        if col_name in df.columns:\n",
    "            df[col_name] = df[col_name].astype(str).str.upper().str.replace(' ', '_').str.replace('DEBATER_', 'DEBATER_')\n",
    "        return df\n",
    "\n",
    "    llm_scores_df = standardize_debater_name(llm_scores_df)\n",
    "\n",
    "    id_vars = ['debate']\n",
    "    value_vars = [col for col in llm_winners_p1_df.columns if col != 'debate']\n",
    "    llm_winners_p1_long = llm_winners_p1_df.melt(id_vars=id_vars, value_vars=value_vars, var_name='model', value_name='debater')\n",
    "    llm_winners_p1_long = standardize_debater_name(llm_winners_p1_long)\n",
    "    llm_winners_p1_df = llm_winners_p1_long.pivot(index='debate', columns='model', values='debater').reset_index()\n",
    "\n",
    "    self_assessment_ranking_df = standardize_debater_name(self_assessment_ranking_df)\n",
    "    judges_hard_vote_ranking_df = standardize_debater_name(judges_hard_vote_ranking_df)\n",
    "    judges_soft_vote_ranking_df = standardize_debater_name(judges_soft_vote_ranking_df)\n",
    "\n",
    "    llm_total_scores = llm_scores_df.groupby(['prompt', 'debate', 'model', 'debater'])['score'].sum().reset_index()\n",
    "    llm_total_scores['rank'] = llm_total_scores.groupby(['prompt', 'debate', 'model'])['score'].rank(method='dense', ascending=False).astype(int)\n",
    "    llm_ranking_p234 = llm_total_scores.sort_values(by=['prompt', 'debate', 'model', 'rank'])\n",
    "\n",
    "    llm_winners_p1_long = llm_winners_p1_df.melt(id_vars='debate', var_name='model', value_name='debater')\n",
    "    llm_winners_p1_long['prompt'] = 1\n",
    "    \n",
    "    all_debaters = judges_soft_vote_ranking_df[['debate', 'debater']].drop_duplicates()\n",
    "    llm_winners_p1_ranked = pd.merge(all_debaters, llm_winners_p1_long, on=['debate', 'debater'], how='left')\n",
    "    llm_winners_p1_ranked['model'] = llm_winners_p1_ranked.groupby(['debate'])['model'].ffill().bfill()\n",
    "    llm_winners_p1_ranked['rank'] = np.where(llm_winners_p1_ranked['model'].notna(), 1, 2)\n",
    "    \n",
    "    final_llm_rankings = pd.concat([llm_winners_p1_ranked, llm_ranking_p234])\n",
    "    print(\">>> Rankings dos LLMs preparados.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro na preparação dos rankings dos LLMs: {e}\")\n",
    "\n",
    "# --- 3. FUNÇÕES PARA CÁLCULO DAS MÉTRICAS ---\n",
    "\n",
    "def calculate_winners_accuracy(predictions, ground_truth):\n",
    "    pred_winners = predictions[predictions['rank'] == 1]\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(pred_winners, gt_winners[['debate', 'gt_debater']], on='debate')\n",
    "    correct = (merged['debater'] == merged['gt_debater'])\n",
    "    accuracy = correct.groupby([merged['prompt'], merged['model']]).mean().rename('winners_accuracy')\n",
    "    return accuracy\n",
    "    \n",
    "# --- FUNÇÃO CORRIGIDA ---\n",
    "def calculate_debaters_accuracy(predictions, ground_truth):\n",
    "    # Junta as predições e o gabarito\n",
    "    merged = pd.merge(predictions, ground_truth, on=['debate', 'debater'], suffixes=('_pred', '_gt'))\n",
    "    \n",
    "    # Uma predição de rank está correta se o rank previsto for igual ao rank real\n",
    "    merged['is_correct'] = (merged['rank_pred'] == merged['rank_gt'])\n",
    "    \n",
    "    # A acurácia é a média da coluna 'is_correct' (True=1, False=0), agrupada por teste\n",
    "    accuracy = merged.groupby(['prompt', 'model'])['is_correct'].mean()\n",
    "    \n",
    "    return accuracy.rename('debaters_accuracy')\n",
    "\n",
    "def calculate_mrr(predictions, ground_truth):\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(predictions, gt_winners[['debate', 'gt_debater']], on='debate', how='left')\n",
    "    correct_predictions = merged[merged['debater'] == merged['gt_debater']]\n",
    "    first_correct_rank = correct_predictions.groupby(['prompt', 'debate', 'model'])['rank'].min()\n",
    "    mrr = (1 / first_correct_rank).groupby(['prompt', 'model']).mean().rename('mrr')\n",
    "    return mrr\n",
    "\n",
    "def calculate_ndcg(predictions, ground_truth):\n",
    "    ground_truth['relevance'] = 1 / ground_truth['rank']\n",
    "    merged = pd.merge(predictions, ground_truth[['debate', 'debater', 'relevance']], on=['debate', 'debater'], how='left').fillna(0)\n",
    "    \n",
    "    results = {}\n",
    "    for name, group in merged.groupby(['prompt', 'model', 'debate']):\n",
    "        prompt, model, debate = name\n",
    "        if len(group) < 2: continue\n",
    "        \n",
    "        true_relevance = np.asarray([group.sort_values(by='relevance', ascending=False)['relevance'].values])\n",
    "        predicted_relevance = np.asarray([group.sort_values(by='rank')['relevance'].values])\n",
    "        \n",
    "        ndcg = ndcg_score(true_relevance, predicted_relevance)\n",
    "        if (prompt, model) not in results: results[(prompt, model)] = []\n",
    "        results[(prompt, model)].append(ndcg)\n",
    "        \n",
    "    final_ndcg = {k: np.mean(v) for k, v in results.items()}\n",
    "    return pd.Series(final_ndcg, name='ndcg').rename_axis(['prompt', 'model'])\n",
    "\n",
    "# --- 4. EXECUÇÃO DAS ANÁLISES ---\n",
    "print(\"\\n[ETAPA 3/4] Calculando as métricas finais...\")\n",
    "\n",
    "ground_truths = {\n",
    "    \"vs_SelfAssessment\": self_assessment_ranking_df,\n",
    "    \"vs_Judges_HardVote\": judges_hard_vote_ranking_df,\n",
    "    \"vs_Judges_SoftVote\": judges_soft_vote_ranking_df\n",
    "}\n",
    "final_results_list = []\n",
    "\n",
    "for gt_name, gt_df in ground_truths.items():\n",
    "    common_debates = gt_df['debate'].unique()\n",
    "    predictions_filtered = final_llm_rankings[final_llm_rankings['debate'].isin(common_debates)]\n",
    "        \n",
    "    acc = calculate_winners_accuracy(predictions_filtered, gt_df)\n",
    "    debaters_acc = calculate_debaters_accuracy(predictions_filtered, gt_df)\n",
    "    mrr = calculate_mrr(predictions_filtered, gt_df)\n",
    "    ndcg = calculate_ndcg(predictions_filtered, gt_df)\n",
    "    \n",
    "    result_df = pd.concat([acc, debaters_acc, mrr, ndcg], axis=1)\n",
    "    result_df['comparison'] = gt_name\n",
    "    final_results_list.append(result_df)\n",
    "\n",
    "final_summary_df = pd.concat(final_results_list).reset_index()\n",
    "final_summary_df = final_summary_df.set_index(['comparison', 'prompt', 'model']).sort_index()\n",
    "\n",
    "# --- 5. EXIBIÇÃO E SALVAMENTO ---\n",
    "print(\"\\n[ETAPA 4/4] Tabela de Resultados Finais:\")\n",
    "display(final_summary_df)\n",
    "\n",
    "final_summary_df.to_csv('final_summary_results.csv')\n",
    "print(\"\\nTabela de resultados finais salva em 'final_summary_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ffc361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando a Análise Final e Cálculo de Métricas (v4 - com Kendall e Spearman) ---\n",
      "\n",
      "[ETAPA 1/5] Carregando e preparando todos os dataframes...\n",
      ">>> Dados carregados e preparados com sucesso.\n",
      "\n",
      "[ETAPA 3/5] Calculando as métricas finais (incluindo Kendall e Spearman)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Klaywert\\AppData\\Local\\Temp\\ipykernel_24976\\3281752782.py:71: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = spearmanr(group['rank_pred'], group['rank_gt'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ETAPA 4/5] Tabela de Resultados Finais:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>winners_accuracy</th>\n",
       "      <th>debaters_accuracy</th>\n",
       "      <th>mrr</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>kendall_tau</th>\n",
       "      <th>spearman_rho</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comparison</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">vs_Judges_HardVote</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.982091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.925408</td>\n",
       "      <td>0.592781</td>\n",
       "      <td>0.674839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>0.953680</td>\n",
       "      <td>0.630921</td>\n",
       "      <td>0.680794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.689583</td>\n",
       "      <td>0.908007</td>\n",
       "      <td>0.336921</td>\n",
       "      <td>0.365731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.951635</td>\n",
       "      <td>0.618296</td>\n",
       "      <td>0.715615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.929081</td>\n",
       "      <td>0.500940</td>\n",
       "      <td>0.570866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.931477</td>\n",
       "      <td>0.576630</td>\n",
       "      <td>0.635609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.752186</td>\n",
       "      <td>0.505294</td>\n",
       "      <td>0.608390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.857898</td>\n",
       "      <td>0.688715</td>\n",
       "      <td>0.764547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.917397</td>\n",
       "      <td>0.434516</td>\n",
       "      <td>0.488749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">vs_Judges_SoftVote</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.904995</td>\n",
       "      <td>0.490388</td>\n",
       "      <td>0.615543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.776042</td>\n",
       "      <td>0.932367</td>\n",
       "      <td>0.522632</td>\n",
       "      <td>0.616557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.910756</td>\n",
       "      <td>0.462120</td>\n",
       "      <td>0.517909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.927046</td>\n",
       "      <td>0.523721</td>\n",
       "      <td>0.653043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.692708</td>\n",
       "      <td>0.907318</td>\n",
       "      <td>0.415388</td>\n",
       "      <td>0.528043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.713542</td>\n",
       "      <td>0.915760</td>\n",
       "      <td>0.540388</td>\n",
       "      <td>0.615543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.751438</td>\n",
       "      <td>0.618455</td>\n",
       "      <td>0.712836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.847582</td>\n",
       "      <td>0.651969</td>\n",
       "      <td>0.749912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.899806</td>\n",
       "      <td>0.460805</td>\n",
       "      <td>0.518862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">vs_SelfAssessment</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.942622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.945544</td>\n",
       "      <td>0.578657</td>\n",
       "      <td>0.626690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.953326</td>\n",
       "      <td>0.568818</td>\n",
       "      <td>0.604983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.932175</td>\n",
       "      <td>0.329030</td>\n",
       "      <td>0.363345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.570250</td>\n",
       "      <td>0.620516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.338235</td>\n",
       "      <td>0.817708</td>\n",
       "      <td>0.944069</td>\n",
       "      <td>0.466854</td>\n",
       "      <td>0.529822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.958577</td>\n",
       "      <td>0.458847</td>\n",
       "      <td>0.523090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4.0</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.773167</td>\n",
       "      <td>0.567198</td>\n",
       "      <td>0.619538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.855767</td>\n",
       "      <td>0.575612</td>\n",
       "      <td>0.631353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.951829</td>\n",
       "      <td>0.505680</td>\n",
       "      <td>0.568844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  winners_accuracy  debaters_accuracy  \\\n",
       "comparison         prompt model                                         \n",
       "vs_Judges_HardVote 0.0    0                    NaN                NaN   \n",
       "                   1.0    claude          0.555556           0.411765   \n",
       "                          gemini          0.705882           0.455882   \n",
       "                          gpt4o           0.444444           0.382353   \n",
       "                   2.0    claude          0.588235           0.411765   \n",
       "                          gemini          0.529412           0.397059   \n",
       "                          gpt4o           0.529412           0.411765   \n",
       "                   4.0    claude          0.444444           0.375000   \n",
       "                          gemini          0.588235           0.375000   \n",
       "                          gpt4o           0.555556           0.455882   \n",
       "vs_Judges_SoftVote 0.0    0                    NaN                NaN   \n",
       "                   1.0    claude          0.470588           0.397059   \n",
       "                          gemini          0.625000           0.455882   \n",
       "                          gpt4o           0.529412           0.426471   \n",
       "                   2.0    claude          0.500000           0.426471   \n",
       "                          gemini          0.500000           0.411765   \n",
       "                          gpt4o           0.500000           0.485294   \n",
       "                   4.0    claude          0.470588           0.392857   \n",
       "                          gemini          0.562500           0.453125   \n",
       "                          gpt4o           0.529412           0.470588   \n",
       "vs_SelfAssessment  0.0    0                    NaN                NaN   \n",
       "                   1.0    claude          0.583333           0.397059   \n",
       "                          gemini          0.521739           0.426471   \n",
       "                          gpt4o           0.375000           0.308824   \n",
       "                   2.0    claude          0.608696           0.397059   \n",
       "                          gemini          0.478261           0.338235   \n",
       "                          gpt4o           0.565217           0.352941   \n",
       "                   4.0    claude          0.400000           0.321429   \n",
       "                          gemini          0.391304           0.312500   \n",
       "                          gpt4o           0.520000           0.470588   \n",
       "\n",
       "                                       mrr      ndcg  kendall_tau  \\\n",
       "comparison         prompt model                                     \n",
       "vs_Judges_HardVote 0.0    0            NaN  0.982091          NaN   \n",
       "                   1.0    claude  0.791667  0.925408     0.592781   \n",
       "                          gemini  0.848958  0.953680     0.630921   \n",
       "                          gpt4o   0.689583  0.908007     0.336921   \n",
       "                   2.0    claude  0.802083  0.951635     0.618296   \n",
       "                          gemini  0.734375  0.929081     0.500940   \n",
       "                          gpt4o   0.734375  0.931477     0.576630   \n",
       "                   4.0    claude  0.794872  0.752186     0.505294   \n",
       "                          gemini  0.811111  0.857898     0.688715   \n",
       "                          gpt4o   0.765625  0.917397     0.434516   \n",
       "vs_Judges_SoftVote 0.0    0            NaN  1.000000          NaN   \n",
       "                   1.0    claude  0.718750  0.904995     0.490388   \n",
       "                          gemini  0.776042  0.932367     0.522632   \n",
       "                          gpt4o   0.731250  0.910756     0.462120   \n",
       "                   2.0    claude  0.729167  0.927046     0.523721   \n",
       "                          gemini  0.692708  0.907318     0.415388   \n",
       "                          gpt4o   0.713542  0.915760     0.540388   \n",
       "                   4.0    claude  0.794872  0.751438     0.618455   \n",
       "                          gemini  0.772222  0.847582     0.651969   \n",
       "                          gpt4o   0.744792  0.899806     0.460805   \n",
       "vs_SelfAssessment  0.0    0            NaN  0.942622          NaN   \n",
       "                   1.0    claude  0.921875  0.945544     0.578657   \n",
       "                          gemini  0.859375  0.953326     0.568818   \n",
       "                          gpt4o   0.770833  0.932175     0.329030   \n",
       "                   2.0    claude  0.921875  0.964037     0.570250   \n",
       "                          gemini  0.817708  0.944069     0.466854   \n",
       "                          gpt4o   0.890625  0.958577     0.458847   \n",
       "                   4.0    claude  0.865385  0.773167     0.567198   \n",
       "                          gemini  0.777778  0.855767     0.575612   \n",
       "                          gpt4o   0.890625  0.951829     0.505680   \n",
       "\n",
       "                                  spearman_rho  \n",
       "comparison         prompt model                 \n",
       "vs_Judges_HardVote 0.0    0                NaN  \n",
       "                   1.0    claude      0.674839  \n",
       "                          gemini      0.680794  \n",
       "                          gpt4o       0.365731  \n",
       "                   2.0    claude      0.715615  \n",
       "                          gemini      0.570866  \n",
       "                          gpt4o       0.635609  \n",
       "                   4.0    claude      0.608390  \n",
       "                          gemini      0.764547  \n",
       "                          gpt4o       0.488749  \n",
       "vs_Judges_SoftVote 0.0    0                NaN  \n",
       "                   1.0    claude      0.615543  \n",
       "                          gemini      0.616557  \n",
       "                          gpt4o       0.517909  \n",
       "                   2.0    claude      0.653043  \n",
       "                          gemini      0.528043  \n",
       "                          gpt4o       0.615543  \n",
       "                   4.0    claude      0.712836  \n",
       "                          gemini      0.749912  \n",
       "                          gpt4o       0.518862  \n",
       "vs_SelfAssessment  0.0    0                NaN  \n",
       "                   1.0    claude      0.626690  \n",
       "                          gemini      0.604983  \n",
       "                          gpt4o       0.363345  \n",
       "                   2.0    claude      0.620516  \n",
       "                          gemini      0.529822  \n",
       "                          gpt4o       0.523090  \n",
       "                   4.0    claude      0.619538  \n",
       "                          gemini      0.631353  \n",
       "                          gpt4o       0.568844  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabela de resultados finais salva em 'final_summary_results_full.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import kendalltau, spearmanr # <-- Novo import\n",
    "\n",
    "print(\"--- Iniciando a Análise Final e Cálculo de Métricas (v4 - com Kendall e Spearman) ---\")\n",
    "\n",
    "# --- 1. CARREGAMENTO E PREPARAÇÃO DOS DADOS ---\n",
    "# (Esta parte assume que os CSVs já foram carregados e os DFs preparados como na célula anterior)\n",
    "print(\"\\n[ETAPA 1/5] Carregando e preparando todos os dataframes...\")\n",
    "# ... (código das seções 1 e 2 da resposta anterior para carregar e preparar os DFs) ...\n",
    "# Vou omitir o código completo para ser breve, mas ele deve estar aqui.\n",
    "# Certifique-se que os DFs: final_llm_rankings, self_assessment_ranking_df, \n",
    "# judges_hard_vote_ranking_df, e judges_soft_vote_ranking_df estão carregados.\n",
    "print(\">>> Dados carregados e preparados com sucesso.\")\n",
    "\n",
    "\n",
    "# --- 2. FUNÇÕES PARA CÁLCULO DAS MÉTRICAS (COM ADIÇÕES) ---\n",
    "\n",
    "def calculate_winners_accuracy(predictions, ground_truth):\n",
    "    pred_winners = predictions[predictions['rank'] == 1]\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(pred_winners, gt_winners[['debate', 'gt_debater']], on='debate')\n",
    "    correct = (merged['debater'] == merged['gt_debater'])\n",
    "    accuracy = correct.groupby([merged['prompt'], merged['model']]).mean().rename('winners_accuracy')\n",
    "    return accuracy\n",
    "    \n",
    "def calculate_debaters_accuracy(predictions, ground_truth):\n",
    "    merged = pd.merge(predictions, ground_truth, on=['debate', 'debater'], suffixes=('_pred', '_gt'))\n",
    "    merged['is_correct'] = (merged['rank_pred'] == merged['rank_gt'])\n",
    "    accuracy = merged.groupby(['prompt', 'model'])['is_correct'].mean()\n",
    "    return accuracy.rename('debaters_accuracy')\n",
    "\n",
    "def calculate_mrr(predictions, ground_truth):\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(predictions, gt_winners[['debate', 'gt_debater']], on='debate', how='left')\n",
    "    correct_predictions = merged[merged['debater'] == merged['gt_debater']]\n",
    "    first_correct_rank = correct_predictions.groupby(['prompt', 'debate', 'model'])['rank'].min()\n",
    "    mrr = (1 / first_correct_rank).groupby(['prompt', 'model']).mean().rename('mrr')\n",
    "    return mrr\n",
    "\n",
    "def calculate_ndcg(predictions, ground_truth):\n",
    "    ground_truth['relevance'] = 1 / ground_truth['rank']\n",
    "    merged = pd.merge(predictions, ground_truth[['debate', 'debater', 'relevance']], on=['debate', 'debater'], how='left').fillna(0)\n",
    "    results = {}\n",
    "    for name, group in merged.groupby(['prompt', 'model', 'debate']):\n",
    "        prompt, model, debate = name\n",
    "        if len(group) < 2: continue\n",
    "        true_relevance = np.asarray([group.sort_values(by='relevance', ascending=False)['relevance'].values])\n",
    "        predicted_relevance = np.asarray([group.sort_values(by='rank')['relevance'].values])\n",
    "        ndcg = ndcg_score(true_relevance, predicted_relevance)\n",
    "        if (prompt, model) not in results: results[(prompt, model)] = []\n",
    "        results[(prompt, model)].append(ndcg)\n",
    "    final_ndcg = {k: np.mean(v) for k, v in results.items()}\n",
    "    return pd.Series(final_ndcg, name='ndcg').rename_axis(['prompt', 'model'])\n",
    "\n",
    "# --- NOVAS FUNÇÕES ADICIONADAS AQUI ---\n",
    "def calculate_rank_correlation(predictions, ground_truth, method='kendall'):\n",
    "    \"\"\"Calcula a correlação de rank média (Kendall's Tau ou Spearman's Rho).\"\"\"\n",
    "    # Garante que os dataframes estejam alinhados\n",
    "    merged = pd.merge(predictions, ground_truth, on=['debate', 'debater'], suffixes=('_pred', '_gt'))\n",
    "    \n",
    "    correlations = []\n",
    "    # Itera sobre cada teste (prompt, debate, model)\n",
    "    for name, group in merged.groupby(['prompt', 'model', 'debate']):\n",
    "        # Garante que as listas de rank tenham o mesmo tamanho\n",
    "        if len(group['rank_pred']) == len(group['rank_gt']) and len(group) > 1:\n",
    "            if method == 'kendall':\n",
    "                corr, _ = kendalltau(group['rank_pred'], group['rank_gt'])\n",
    "            elif method == 'spearman':\n",
    "                corr, _ = spearmanr(group['rank_pred'], group['rank_gt'])\n",
    "            else:\n",
    "                corr = np.nan\n",
    "            correlations.append({'prompt': name[0], 'model': name[1], 'correlation': corr})\n",
    "    \n",
    "    # Calcula a média da correlação para cada (prompt, model)\n",
    "    corr_df = pd.DataFrame(correlations)\n",
    "    avg_corr = corr_df.groupby(['prompt', 'model'])['correlation'].mean()\n",
    "    return avg_corr\n",
    "\n",
    "# --- 4. EXECUÇÃO DAS ANÁLISES ---\n",
    "print(\"\\n[ETAPA 3/5] Calculando as métricas finais (incluindo Kendall e Spearman)...\")\n",
    "\n",
    "ground_truths = {\n",
    "    \"vs_SelfAssessment\": self_assessment_ranking_df,\n",
    "    \"vs_Judges_HardVote\": judges_hard_vote_ranking_df,\n",
    "    \"vs_Judges_SoftVote\": judges_soft_vote_ranking_df\n",
    "}\n",
    "final_results_list = []\n",
    "\n",
    "for gt_name, gt_df in ground_truths.items():\n",
    "    common_debates = gt_df['debate'].unique()\n",
    "    predictions_filtered = final_llm_rankings[final_llm_rankings['debate'].isin(common_debates)]\n",
    "        \n",
    "    acc = calculate_winners_accuracy(predictions_filtered, gt_df)\n",
    "    debaters_acc = calculate_debaters_accuracy(predictions_filtered, gt_df)\n",
    "    mrr = calculate_mrr(predictions_filtered, gt_df)\n",
    "    ndcg = calculate_ndcg(predictions_filtered, gt_df)\n",
    "    # --- CHAMADA DAS NOVAS FUNÇÕES ---\n",
    "    kendall = calculate_rank_correlation(predictions_filtered, gt_df, method='kendall').rename('kendall_tau')\n",
    "    spearman = calculate_rank_correlation(predictions_filtered, gt_df, method='spearman').rename('spearman_rho')\n",
    "    \n",
    "    # --- ADIÇÃO DAS NOVAS MÉTRICAS AO RESULTADO ---\n",
    "    result_df = pd.concat([acc, debaters_acc, mrr, ndcg, kendall, spearman], axis=1)\n",
    "    result_df['comparison'] = gt_name\n",
    "    final_results_list.append(result_df)\n",
    "\n",
    "final_summary_df = pd.concat(final_results_list).reset_index()\n",
    "final_summary_df = final_summary_df.set_index(['comparison', 'prompt', 'model']).sort_index()\n",
    "\n",
    "# --- 5. EXIBIÇÃO E SALVAMENTO ---\n",
    "print(\"\\n[ETAPA 4/5] Tabela de Resultados Finais:\")\n",
    "display(final_summary_df)\n",
    "\n",
    "final_summary_df.to_csv('final_summary_results_full.csv')\n",
    "print(\"\\nTabela de resultados finais salva em 'final_summary_results_full.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5996d44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO ANÁLISE COMPLETA E DEFINITIVA ---\n",
      "\n",
      "[ETAPA 1/6] Lendo e processando os arquivos JSON dos LLMs...\n",
      ">>> Leitura de 12259 registros brutos concluída.\n",
      "\n",
      "[ETAPA 2/6] Carregando arquivos de Ground Truth...\n",
      ">>> Dados de Ground Truth carregados com sucesso.\n",
      "\n",
      "[ETAPA 3/6] Preparando e padronizando rankings...\n",
      ">>> Rankings dos LLMs e Ground Truth preparados e padronizados.\n",
      "\n",
      "[ETAPA 4/6] Calculando as métricas finais...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Klaywert\\AppData\\Local\\Temp\\ipykernel_668\\974290657.py:136: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  elif method == 'spearman': corr, _ = spearmanr(group['rank_pred'], group['rank_gt'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ETAPA 5/6] Tabela de Resultados Finais:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>winners_accuracy</th>\n",
       "      <th>debaters_accuracy</th>\n",
       "      <th>mrr</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>kendall_tau</th>\n",
       "      <th>spearman_rho</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comparison</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">vs_Judges_HardVote</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.925408</td>\n",
       "      <td>0.592781</td>\n",
       "      <td>0.674839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>0.953680</td>\n",
       "      <td>0.630921</td>\n",
       "      <td>0.680794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.689583</td>\n",
       "      <td>0.908007</td>\n",
       "      <td>0.336921</td>\n",
       "      <td>0.365731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.951635</td>\n",
       "      <td>0.618296</td>\n",
       "      <td>0.715615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.929081</td>\n",
       "      <td>0.500940</td>\n",
       "      <td>0.570866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.931477</td>\n",
       "      <td>0.576630</td>\n",
       "      <td>0.635609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.919716</td>\n",
       "      <td>0.520181</td>\n",
       "      <td>0.598676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.710714</td>\n",
       "      <td>0.781333</td>\n",
       "      <td>0.476835</td>\n",
       "      <td>0.528962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.870884</td>\n",
       "      <td>0.186197</td>\n",
       "      <td>0.200352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.752186</td>\n",
       "      <td>0.505294</td>\n",
       "      <td>0.608390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.857898</td>\n",
       "      <td>0.688715</td>\n",
       "      <td>0.764547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.917397</td>\n",
       "      <td>0.434516</td>\n",
       "      <td>0.488749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">vs_Judges_SoftVote</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.904995</td>\n",
       "      <td>0.490388</td>\n",
       "      <td>0.615543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.776042</td>\n",
       "      <td>0.932367</td>\n",
       "      <td>0.522632</td>\n",
       "      <td>0.616557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.910756</td>\n",
       "      <td>0.462120</td>\n",
       "      <td>0.517909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.927046</td>\n",
       "      <td>0.523721</td>\n",
       "      <td>0.653043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.692708</td>\n",
       "      <td>0.907318</td>\n",
       "      <td>0.415388</td>\n",
       "      <td>0.528043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.713542</td>\n",
       "      <td>0.915760</td>\n",
       "      <td>0.540388</td>\n",
       "      <td>0.615543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.903890</td>\n",
       "      <td>0.466854</td>\n",
       "      <td>0.570331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>0.698810</td>\n",
       "      <td>0.768232</td>\n",
       "      <td>0.484549</td>\n",
       "      <td>0.537134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.226182</td>\n",
       "      <td>0.240007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.751438</td>\n",
       "      <td>0.618455</td>\n",
       "      <td>0.712836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.847582</td>\n",
       "      <td>0.651969</td>\n",
       "      <td>0.749912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.899806</td>\n",
       "      <td>0.460805</td>\n",
       "      <td>0.518862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">vs_SelfAssessment</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.945544</td>\n",
       "      <td>0.578657</td>\n",
       "      <td>0.626690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.953326</td>\n",
       "      <td>0.568818</td>\n",
       "      <td>0.604983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.932175</td>\n",
       "      <td>0.329030</td>\n",
       "      <td>0.363345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.570250</td>\n",
       "      <td>0.620516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.338235</td>\n",
       "      <td>0.817708</td>\n",
       "      <td>0.944069</td>\n",
       "      <td>0.466854</td>\n",
       "      <td>0.529822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.958577</td>\n",
       "      <td>0.458847</td>\n",
       "      <td>0.523090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>0.934327</td>\n",
       "      <td>0.564289</td>\n",
       "      <td>0.624733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.802364</td>\n",
       "      <td>0.404729</td>\n",
       "      <td>0.459200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.912626</td>\n",
       "      <td>0.343740</td>\n",
       "      <td>0.373243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>claude</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.773167</td>\n",
       "      <td>0.567198</td>\n",
       "      <td>0.619538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.855767</td>\n",
       "      <td>0.575612</td>\n",
       "      <td>0.631353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.951829</td>\n",
       "      <td>0.505680</td>\n",
       "      <td>0.568844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  winners_accuracy  debaters_accuracy  \\\n",
       "comparison         prompt model                                         \n",
       "vs_Judges_HardVote 1      claude          0.555556           0.411765   \n",
       "                          gemini          0.705882           0.455882   \n",
       "                          gpt4o           0.444444           0.382353   \n",
       "                   2      claude          0.588235           0.411765   \n",
       "                          gemini          0.529412           0.397059   \n",
       "                          gpt4o           0.529412           0.411765   \n",
       "                   3      claude          0.500000           0.323529   \n",
       "                          gemini          0.444444           0.355932   \n",
       "                          gpt4o           0.333333           0.279412   \n",
       "                   4      claude          0.444444           0.375000   \n",
       "                          gemini          0.588235           0.375000   \n",
       "                          gpt4o           0.555556           0.455882   \n",
       "vs_Judges_SoftVote 1      claude          0.470588           0.397059   \n",
       "                          gemini          0.625000           0.455882   \n",
       "                          gpt4o           0.529412           0.426471   \n",
       "                   2      claude          0.500000           0.426471   \n",
       "                          gemini          0.500000           0.411765   \n",
       "                          gpt4o           0.500000           0.485294   \n",
       "                   3      claude          0.529412           0.352941   \n",
       "                          gemini          0.470588           0.406780   \n",
       "                          gpt4o           0.391304           0.264706   \n",
       "                   4      claude          0.470588           0.392857   \n",
       "                          gemini          0.562500           0.453125   \n",
       "                          gpt4o           0.529412           0.470588   \n",
       "vs_SelfAssessment  1      claude          0.583333           0.397059   \n",
       "                          gemini          0.521739           0.426471   \n",
       "                          gpt4o           0.375000           0.308824   \n",
       "                   2      claude          0.608696           0.397059   \n",
       "                          gemini          0.478261           0.338235   \n",
       "                          gpt4o           0.565217           0.352941   \n",
       "                   3      claude          0.481481           0.397059   \n",
       "                          gemini          0.333333           0.322034   \n",
       "                          gpt4o           0.393939           0.485294   \n",
       "                   4      claude          0.400000           0.321429   \n",
       "                          gemini          0.391304           0.312500   \n",
       "                          gpt4o           0.520000           0.470588   \n",
       "\n",
       "                                       mrr      ndcg  kendall_tau  \\\n",
       "comparison         prompt model                                     \n",
       "vs_Judges_HardVote 1      claude  0.791667  0.925408     0.592781   \n",
       "                          gemini  0.848958  0.953680     0.630921   \n",
       "                          gpt4o   0.689583  0.908007     0.336921   \n",
       "                   2      claude  0.802083  0.951635     0.618296   \n",
       "                          gemini  0.734375  0.929081     0.500940   \n",
       "                          gpt4o   0.734375  0.931477     0.576630   \n",
       "                   3      claude  0.750000  0.919716     0.520181   \n",
       "                          gemini  0.710714  0.781333     0.476835   \n",
       "                          gpt4o   0.729167  0.870884     0.186197   \n",
       "                   4      claude  0.794872  0.752186     0.505294   \n",
       "                          gemini  0.811111  0.857898     0.688715   \n",
       "                          gpt4o   0.765625  0.917397     0.434516   \n",
       "vs_Judges_SoftVote 1      claude  0.718750  0.904995     0.490388   \n",
       "                          gemini  0.776042  0.932367     0.522632   \n",
       "                          gpt4o   0.731250  0.910756     0.462120   \n",
       "                   2      claude  0.729167  0.927046     0.523721   \n",
       "                          gemini  0.692708  0.907318     0.415388   \n",
       "                          gpt4o   0.713542  0.915760     0.540388   \n",
       "                   3      claude  0.739583  0.903890     0.466854   \n",
       "                          gemini  0.698810  0.768232     0.484549   \n",
       "                          gpt4o   0.760417  0.850610     0.226182   \n",
       "                   4      claude  0.794872  0.751438     0.618455   \n",
       "                          gemini  0.772222  0.847582     0.651969   \n",
       "                          gpt4o   0.744792  0.899806     0.460805   \n",
       "vs_SelfAssessment  1      claude  0.921875  0.945544     0.578657   \n",
       "                          gemini  0.859375  0.953326     0.568818   \n",
       "                          gpt4o   0.770833  0.932175     0.329030   \n",
       "                   2      claude  0.921875  0.964037     0.570250   \n",
       "                          gemini  0.817708  0.944069     0.466854   \n",
       "                          gpt4o   0.890625  0.958577     0.458847   \n",
       "                   3      claude  0.848958  0.934327     0.564289   \n",
       "                          gemini  0.738095  0.802364     0.404729   \n",
       "                          gpt4o   0.854167  0.912626     0.343740   \n",
       "                   4      claude  0.865385  0.773167     0.567198   \n",
       "                          gemini  0.777778  0.855767     0.575612   \n",
       "                          gpt4o   0.890625  0.951829     0.505680   \n",
       "\n",
       "                                  spearman_rho  \n",
       "comparison         prompt model                 \n",
       "vs_Judges_HardVote 1      claude      0.674839  \n",
       "                          gemini      0.680794  \n",
       "                          gpt4o       0.365731  \n",
       "                   2      claude      0.715615  \n",
       "                          gemini      0.570866  \n",
       "                          gpt4o       0.635609  \n",
       "                   3      claude      0.598676  \n",
       "                          gemini      0.528962  \n",
       "                          gpt4o       0.200352  \n",
       "                   4      claude      0.608390  \n",
       "                          gemini      0.764547  \n",
       "                          gpt4o       0.488749  \n",
       "vs_Judges_SoftVote 1      claude      0.615543  \n",
       "                          gemini      0.616557  \n",
       "                          gpt4o       0.517909  \n",
       "                   2      claude      0.653043  \n",
       "                          gemini      0.528043  \n",
       "                          gpt4o       0.615543  \n",
       "                   3      claude      0.570331  \n",
       "                          gemini      0.537134  \n",
       "                          gpt4o       0.240007  \n",
       "                   4      claude      0.712836  \n",
       "                          gemini      0.749912  \n",
       "                          gpt4o       0.518862  \n",
       "vs_SelfAssessment  1      claude      0.626690  \n",
       "                          gemini      0.604983  \n",
       "                          gpt4o       0.363345  \n",
       "                   2      claude      0.620516  \n",
       "                          gemini      0.529822  \n",
       "                          gpt4o       0.523090  \n",
       "                   3      claude      0.624733  \n",
       "                          gemini      0.459200  \n",
       "                          gpt4o       0.373243  \n",
       "                   4      claude      0.619538  \n",
       "                          gemini      0.631353  \n",
       "                          gpt4o       0.568844  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ETAPA 6/6] Salvando tabela de resultados...\n",
      "\n",
      "Tabela de resultados finais salva em 'final_summary_results_full.csv'\n",
      "\n",
      "--- Análise concluída! ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "\n",
    "print(\"--- INICIANDO ANÁLISE COMPLETA E DEFINITIVA ---\")\n",
    "\n",
    "# --- 1. CONFIGURAÇÃO ---\n",
    "OUTPUTS_DIR = 'outputs'\n",
    "SELF_ASSESSMENT_PATH = 'gt_self_assessment_ranking.csv'\n",
    "JUDGES_HARD_VOTE_PATH = 'gt_judges_hard_vote_ranking.csv'\n",
    "JUDGES_SOFT_VOTE_PATH = 'gt_judges_soft_vote_ranking.csv'\n",
    "\n",
    "# --- 2. CARREGAMENTO E PROCESSAMENTO DOS DADOS BRUTOS DOS LLMs ---\n",
    "print(\"\\n[ETAPA 1/6] Lendo e processando os arquivos JSON dos LLMs...\")\n",
    "all_runs_data = []\n",
    "try:\n",
    "    for root, dirs, files in os.walk(OUTPUTS_DIR):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    parts = file_path.split(os.sep)\n",
    "                    prompt_num = int(parts[1].split('_')[1])\n",
    "                    debate_num = int(parts[2].split('_')[1])\n",
    "                    model_name = filename.split('_')[0]\n",
    "                    run_num = int(filename.split('_')[2].split('.')[0])\n",
    "                except (ValueError, IndexError): continue\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                if \"error\" in data: continue\n",
    "\n",
    "                for debater in data.get('debaters', []):\n",
    "                    debater_name = debater.get('name')\n",
    "                    performance_analysis = debater.get('performance', {}).get('performance_analysis', debater.get('performance_evaluation', ''))\n",
    "                    \n",
    "                    if prompt_num == 1:\n",
    "                        score = debater.get('overall_score')\n",
    "                        if score is not None:\n",
    "                            all_runs_data.append({'prompt': 1, 'debate': debate_num, 'model': model_name, 'run': run_num, 'debater': debater_name, 'criterion': 'overall_score', 'score': score, 'analysis_text': performance_analysis})\n",
    "                    elif prompt_num == 3:\n",
    "                        positive_events = debater.get('positive_events', {})\n",
    "                        negative_events = debater.get('negative_events', {})\n",
    "                        score = sum(len(v) for v in positive_events.values()) - sum(len(v) for v in negative_events.values())\n",
    "                        all_runs_data.append({'prompt': 3, 'debate': debate_num, 'model': model_name, 'run': run_num, 'debater': debater_name, 'criterion': 'total_event_score', 'score': score, 'analysis_text': performance_analysis})\n",
    "                    else: # Prompts 2 e 4\n",
    "                        scores_data = debater.get('scores', {})\n",
    "                        if not scores_data: scores_data = {k: v.get('score') for k, v in debater.get('evaluation_aspects', {}).items()}\n",
    "                        for criterion, score in scores_data.items():\n",
    "                            all_runs_data.append({'prompt': prompt_num, 'debate': debate_num, 'model': model_name, 'run': run_num, 'debater': debater_name, 'criterion': criterion, 'score': score, 'analysis_text': performance_analysis})\n",
    "    \n",
    "    raw_df = pd.DataFrame(all_runs_data)\n",
    "    print(f\">>> Leitura de {len(raw_df)} registros brutos concluída.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO na leitura dos JSONs: {e}\")\n",
    "\n",
    "\n",
    "# --- 3. CARREGAMENTO DOS DADOS DE GROUND TRUTH ---\n",
    "print(\"\\n[ETAPA 2/6] Carregando arquivos de Ground Truth...\")\n",
    "try:\n",
    "    self_assessment_ranking_df = pd.read_csv(SELF_ASSESSMENT_PATH)\n",
    "    judges_hard_vote_ranking_df = pd.read_csv(JUDGES_HARD_VOTE_PATH)\n",
    "    judges_soft_vote_ranking_df = pd.read_csv(JUDGES_SOFT_VOTE_PATH)\n",
    "    print(\">>> Dados de Ground Truth carregados com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao carregar arquivos de Ground Truth: {e}\")\n",
    "\n",
    "# --- 4. PREPARAÇÃO E PADRONIZAÇÃO FINAL DOS RANKINGS ---\n",
    "print(\"\\n[ETAPA 3/6] Preparando e padronizando rankings...\")\n",
    "try:\n",
    "    def standardize_debater_name(df, col_name='debater'):\n",
    "        if col_name in df.columns:\n",
    "            df[col_name] = df[col_name].astype(str).str.upper().str.replace(' ', '_').str.replace('DEBATER_', 'DEBATER_')\n",
    "        return df\n",
    "\n",
    "    raw_df = standardize_debater_name(raw_df)\n",
    "    self_assessment_ranking_df = standardize_debater_name(self_assessment_ranking_df)\n",
    "    judges_hard_vote_ranking_df = standardize_debater_name(judges_hard_vote_ranking_df)\n",
    "    judges_soft_vote_ranking_df = standardize_debater_name(judges_soft_vote_ranking_df)\n",
    "\n",
    "    agg_df = raw_df.groupby(['prompt', 'debate', 'model', 'debater', 'criterion'])['score'].mean().reset_index()\n",
    "    total_scores = agg_df.groupby(['prompt', 'debate', 'model', 'debater'])['score'].sum().reset_index()\n",
    "    total_scores['rank'] = total_scores.groupby(['prompt', 'debate', 'model'])['score'].rank(method='dense', ascending=False).astype(int)\n",
    "    final_llm_rankings = total_scores.sort_values(by=['prompt', 'debate', 'model', 'rank'])\n",
    "    print(\">>> Rankings dos LLMs e Ground Truth preparados e padronizados.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro na preparação dos rankings: {e}\")\n",
    "\n",
    "# --- 5. FUNÇÕES DE MÉTRICA ---\n",
    "def calculate_winners_accuracy(predictions, ground_truth):\n",
    "    pred_winners = predictions[predictions['rank'] == 1]\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(pred_winners, gt_winners[['debate', 'gt_debater']], on='debate')\n",
    "    correct = (merged['debater'] == merged['gt_debater'])\n",
    "    return correct.groupby([merged['prompt'], merged['model']]).mean().rename('winners_accuracy')\n",
    "\n",
    "def calculate_debaters_accuracy(predictions, ground_truth):\n",
    "    merged = pd.merge(predictions, ground_truth, on=['debate', 'debater'], suffixes=('_pred', '_gt'))\n",
    "    merged['is_correct'] = (merged['rank_pred'] == merged['rank_gt'])\n",
    "    return merged.groupby(['prompt', 'model'])['is_correct'].mean().rename('debaters_accuracy')\n",
    "\n",
    "def calculate_mrr(predictions, ground_truth):\n",
    "    gt_winners = ground_truth[ground_truth['rank'] == 1].rename(columns={'debater': 'gt_debater'})\n",
    "    merged = pd.merge(predictions, gt_winners[['debate', 'gt_debater']], on='debate', how='left')\n",
    "    correct_predictions = merged[merged['debater'] == merged['gt_debater']]\n",
    "    first_correct_rank = correct_predictions.groupby(['prompt', 'debate', 'model'])['rank'].min()\n",
    "    return (1 / first_correct_rank).groupby(['prompt', 'model']).mean().rename('mrr')\n",
    "\n",
    "def calculate_ndcg(predictions, ground_truth):\n",
    "    ground_truth['relevance'] = 1 / ground_truth['rank']\n",
    "    merged = pd.merge(predictions, ground_truth[['debate', 'debater', 'relevance']], on=['debate', 'debater'], how='left').fillna(0)\n",
    "    results = {}\n",
    "    for name, group in merged.groupby(['prompt', 'model', 'debate']):\n",
    "        prompt, model, debate = name\n",
    "        if len(group) < 2: continue\n",
    "        true_relevance = np.asarray([group.sort_values(by='relevance', ascending=False)['relevance'].values])\n",
    "        predicted_relevance = np.asarray([group.sort_values(by='rank')['relevance'].values])\n",
    "        ndcg = ndcg_score(true_relevance, predicted_relevance)\n",
    "        if (prompt, model) not in results: results[(prompt, model)] = []\n",
    "        results[(prompt, model)].append(ndcg)\n",
    "    final_ndcg = {k: np.mean(v) for k, v in results.items()}\n",
    "    return pd.Series(final_ndcg, name='ndcg').rename_axis(['prompt', 'model'])\n",
    "\n",
    "def calculate_rank_correlation(predictions, ground_truth, method='kendall'):\n",
    "    merged = pd.merge(predictions, ground_truth, on=['debate', 'debater'], suffixes=('_pred', '_gt'))\n",
    "    correlations = []\n",
    "    for name, group in merged.groupby(['prompt', 'model', 'debate']):\n",
    "        if len(group['rank_pred']) == len(group['rank_gt']) and len(group) > 1:\n",
    "            if method == 'kendall': corr, _ = kendalltau(group['rank_pred'], group['rank_gt'])\n",
    "            elif method == 'spearman': corr, _ = spearmanr(group['rank_pred'], group['rank_gt'])\n",
    "            else: corr = np.nan\n",
    "            correlations.append({'prompt': name[0], 'model': name[1], 'correlation': corr})\n",
    "    corr_df = pd.DataFrame(correlations)\n",
    "    return corr_df.groupby(['prompt', 'model'])['correlation'].mean()\n",
    "\n",
    "\n",
    "# --- 6. EXECUÇÃO FINAL E EXIBIÇÃO ---\n",
    "print(\"\\n[ETAPA 4/6] Calculando as métricas finais...\")\n",
    "ground_truths = {\n",
    "    \"vs_SelfAssessment\": self_assessment_ranking_df,\n",
    "    \"vs_Judges_HardVote\": judges_hard_vote_ranking_df,\n",
    "    \"vs_Judges_SoftVote\": judges_soft_vote_ranking_df\n",
    "}\n",
    "final_results_list = []\n",
    "\n",
    "for gt_name, gt_df in ground_truths.items():\n",
    "    common_debates = gt_df['debate'].unique()\n",
    "    predictions_filtered = final_llm_rankings[final_llm_rankings['debate'].isin(common_debates)]\n",
    "    \n",
    "    acc = calculate_winners_accuracy(predictions_filtered, gt_df)\n",
    "    debaters_acc = calculate_debaters_accuracy(predictions_filtered, gt_df)\n",
    "    mrr = calculate_mrr(predictions_filtered, gt_df)\n",
    "    ndcg = calculate_ndcg(predictions_filtered, gt_df)\n",
    "    kendall = calculate_rank_correlation(predictions_filtered, gt_df, method='kendall').rename('kendall_tau')\n",
    "    spearman = calculate_rank_correlation(predictions_filtered, gt_df, method='spearman').rename('spearman_rho')\n",
    "    \n",
    "    result_df = pd.concat([acc, debaters_acc, mrr, ndcg, kendall, spearman], axis=1)\n",
    "    result_df['comparison'] = gt_name\n",
    "    final_results_list.append(result_df)\n",
    "\n",
    "final_summary_df = pd.concat(final_results_list).reset_index()\n",
    "final_summary_df = final_summary_df.set_index(['comparison', 'prompt', 'model']).sort_index()\n",
    "\n",
    "print(\"\\n[ETAPA 5/6] Tabela de Resultados Finais:\")\n",
    "display(final_summary_df)\n",
    "\n",
    "print(\"\\n[ETAPA 6/6] Salvando tabela de resultados...\")\n",
    "final_summary_df.to_csv('final_summary_results_full.csv')\n",
    "print(\"\\nTabela de resultados finais salva em 'final_summary_results_full.csv'\")\n",
    "print(\"\\n--- Análise concluída! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e874055d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
