{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a33f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Klaywert\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aecff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1 carregado.\n",
      "Prompt 2 carregado.\n",
      "Prompt 3 carregado.\n",
      "Prompt 4 carregado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executando Bateria de Testes:  97%|█████████▋| 186/192 [08:27<02:39, 26.56s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVISO: A resposta não é um JSON válido. Salvando como texto bruto.\n",
      "Resposta recebida:\n",
      "```json\n",
      "{\n",
      "  \"debaters\": [\n",
      "    {\n",
      "      \"name\": \"Debater 1\",\n",
      "      \"evaluation_aspects\": {\n",
      "        \"organization_and_clarity\": {\n",
      "          \"score\": 4,\n",
      "          \"occurrences\": [\n",
      "            \"Initial sta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executando Bateria de Testes:  98%|█████████▊| 188/192 [11:44<03:05, 46.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVISO: A resposta não é um JSON válido. Salvando como texto bruto.\n",
      "Resposta recebida:\n",
      "```json\n",
      "{\n",
      "  \"debaters\": [\n",
      "    {\n",
      "      \"name\": \"Debater 1\",\n",
      "      \"evaluation_aspects\": {\n",
      "        \"organization_and_clarity\": {\n",
      "          \"score\": 4,\n",
      "          \"occurrences\": [\n",
      "            \"Initial sta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executando Bateria de Testes:  98%|█████████▊| 189/192 [13:24<02:49, 56.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVISO: A resposta não é um JSON válido. Salvando como texto bruto.\n",
      "Resposta recebida:\n",
      "```json\n",
      "{\n",
      "  \"debaters\": [\n",
      "    {\n",
      "      \"name\": \"Debater 1\",\n",
      "      \"evaluation_aspects\": {\n",
      "        \"organization_and_clarity\": {\n",
      "          \"score\": 3,\n",
      "          \"occurrences\": [\n",
      "            \"Opening sta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executando Bateria de Testes:  99%|█████████▉| 190/192 [15:02<02:10, 65.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVISO: A resposta não é um JSON válido. Salvando como texto bruto.\n",
      "Resposta recebida:\n",
      "```json\n",
      "{\n",
      "  \"debaters\": [\n",
      "    {\n",
      "      \"name\": \"Debater 1\",\n",
      "      \"evaluation_aspects\": {\n",
      "        \"organization_and_clarity\": {\n",
      "          \"score\": 3,\n",
      "          \"occurrences\": [\n",
      "            \"Opening sta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executando Bateria de Testes: 100%|██████████| 192/192 [18:24<00:00,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bateria de testes completa!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. CONFIGURAÇÃO ---\n",
    "# Cole suas API Keys aqui\n",
    "OPENAI_API_KEY = ''\n",
    "GOOGLE_API_KEY = ''\n",
    "ANTHROPIC_API_KEY = '' \n",
    "\n",
    "# Clientes das APIs\n",
    "client_openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "client_anthropic = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "# Nomes dos modelos\n",
    "MODEL_GPT = 'gpt-4o'\n",
    "MODEL_GEMINI = 'gemini-2.5-flash'\n",
    "MODEL_CLAUDE = 'claude-opus-4-1-20250805'\n",
    "\n",
    "# Definição das pastas\n",
    "PROMPTS_DIR = 'prompts'\n",
    "DEBATES_DIR = 'debates'\n",
    "OUTPUTS_DIR = 'outputs'\n",
    "\n",
    "# Parâmetros do experimento\n",
    "NUM_PROMPTS = 4\n",
    "debate_numbers = [i for i in range(1, 19) if i not in [4, 15]]\n",
    "NUM_RUNS = 3\n",
    "\n",
    "# Configurações para as chamadas das APIs (temperatura 0.0)\n",
    "generation_config_google = {\"temperature\": 0.0}\n",
    "temperature_gpt_claude = 0.0\n",
    "\n",
    "# --- 2. FUNÇÕES AUXILIARES ---\n",
    "\n",
    "def format_debate_csv(file_path):\n",
    "    \"\"\"Lê um arquivo CSV de debate e formata em uma string única.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['debater'] = df['debater'].str.upper()\n",
    "        combined_transcriptions = df.apply(lambda row: f\"- {row['debater']}: {row['transcription']}\", axis=1)\n",
    "        return \"\\n\".join(combined_transcriptions)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler ou formatar o debate {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_json_from_response(text):\n",
    "    \"\"\"Extrai um bloco de código JSON da resposta do LLM.\"\"\"\n",
    "    try:\n",
    "        if '```json' in text:\n",
    "            start = text.find('```json') + len('```json')\n",
    "            end = text.rfind('```')\n",
    "            if start < end:\n",
    "                json_str = text[start:end].strip()\n",
    "                return json.loads(json_str)\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"AVISO: A resposta não é um JSON válido. Salvando como texto bruto.\\nResposta recebida:\\n{text[:200]}...\")\n",
    "        return {\"error\": \"Invalid JSON response\", \"raw_response\": text}\n",
    "    except Exception as e:\n",
    "        print(f\"Erro inesperado ao extrair JSON: {e}\")\n",
    "        return {\"error\": f\"Unexpected error extracting JSON: {e}\", \"raw_response\": text}\n",
    "\n",
    "def call_api(model_name, prompt):\n",
    "    \"\"\"Função genérica para chamar as diferentes APIs.\"\"\"\n",
    "    try:\n",
    "        if model_name == MODEL_GPT:\n",
    "            response = client_openai.chat.completions.create(\n",
    "                model=model_name,\n",
    "                temperature=temperature_gpt_claude,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        elif model_name == MODEL_GEMINI:\n",
    "            model = genai.GenerativeModel(model_name)\n",
    "            response = model.generate_content(prompt, generation_config=generation_config_google)\n",
    "            return response.text\n",
    "        elif model_name == MODEL_CLAUDE: # <-- Lógica para o Claude\n",
    "            message = client_anthropic.messages.create(\n",
    "                model=model_name,\n",
    "                max_tokens=4096, # Claude requer um max_tokens\n",
    "                temperature=temperature_gpt_claude,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return message.content[0].text\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na chamada da API para o modelo {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 3. LÓGICA PRINCIPAL DA BATERIA DE TESTES ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompts_content = {}\n",
    "    for i in range(1, NUM_PROMPTS + 1):\n",
    "        prompt_path = os.path.join(PROMPTS_DIR, f'prompt{i}.txt')\n",
    "        try:\n",
    "            with open(prompt_path, 'r', encoding='utf-8') as f:\n",
    "                prompts_content[i] = f.read()\n",
    "            print(f\"Prompt {i} carregado.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERRO CRÍTICO: Arquivo de prompt não encontrado: {prompt_path}\")\n",
    "            exit()\n",
    "\n",
    "    os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
    "    \n",
    "    models_to_run = {\n",
    "        #\"gpt4o\": MODEL_GPT,\n",
    "        #\"gemini\": MODEL_GEMINI,\n",
    "        \"claude\": MODEL_CLAUDE\n",
    "    }\n",
    "    \n",
    "    total_api_calls = len(prompts_content) * len(debate_numbers) * NUM_RUNS * len(models_to_run)\n",
    "    pbar = tqdm(total=total_api_calls, desc=\"Executando Bateria de Testes\")\n",
    "\n",
    "    for prompt_num in range(1, NUM_PROMPTS + 1):\n",
    "        for debate_num in debate_numbers:\n",
    "            for run_num in range(1, NUM_RUNS + 1):\n",
    "                \n",
    "                output_folder = os.path.join(OUTPUTS_DIR, f'prompt_{prompt_num}', f'debate_{debate_num}')\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "                \n",
    "                debate_path = os.path.join(DEBATES_DIR, f'debate{debate_num}.csv')\n",
    "                debate_text = format_debate_csv(debate_path)\n",
    "                if debate_text is None:\n",
    "                    pbar.update(len(models_to_run))\n",
    "                    continue\n",
    "\n",
    "                prompt_template = prompts_content[prompt_num]\n",
    "                full_prompt = f\"{prompt_template}\\n\\nDEBATE:\\n{debate_text}\"\n",
    "\n",
    "                for model_key, model_name in models_to_run.items():\n",
    "                    output_path = os.path.join(output_folder, f'{model_key}_run_{run_num}.json')\n",
    "                    \n",
    "                    if not os.path.exists(output_path):\n",
    "                        response = call_api(model_name, full_prompt)\n",
    "                        if response:\n",
    "                            json_data = extract_json_from_response(response)\n",
    "                            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                                json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "                        time.sleep(1) \n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    print(\"\\nBateria de testes completa!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca6b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
